{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "916c9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ollama import Client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from TinyRecursiveModels.models.losses import ACTLossHead\n",
    "from TinyRecursiveModels.dataset.build_sudoku_dataset import DataProcessConfig, preprocess_data\n",
    "from TinyRecursiveModels.models.recursive_reasoning.trm import (\n",
    "    TinyRecursiveReasoningModel_ACTV1, \n",
    "    TinyRecursiveReasoningModel_ACTV1Carry, \n",
    "    TinyRecursiveReasoningModel_ACTV1InnerCarry\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a9b28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x212ffac4050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8214eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec553456",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c8f49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ecfed0e52a4b548cd7dc432f13ba56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/719M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:00<00:00, 8419.92it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea564583eaa24f1190b16d609d72890c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/79.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 422786/422786 [00:00<00:00, 1747734.15it/s]\n"
     ]
    }
   ],
   "source": [
    "LAB_DIR = Path(\".\")\n",
    "DATA_ROOT = LAB_DIR / \"data\" / \"sudoku-extreme-mini\"\n",
    "\n",
    "config = DataProcessConfig(\n",
    "    output_dir=str(DATA_ROOT),\n",
    "    subsample_size=512,\n",
    "    min_difficulty=None,\n",
    "    num_aug=1,\n",
    ")\n",
    "\n",
    "train_inputs_path = DATA_ROOT / \"train\" / \"all__inputs.npy\"\n",
    "\n",
    "if not train_inputs_path.exists():\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    preprocess_data(config)\n",
    "else:\n",
    "    print(f\"Using cached dataset at {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_text(tokens, side=None):\n",
    "    arr = np.array(tokens, dtype=np.int64)\n",
    "    if side is not None:\n",
    "        arr = arr.reshape(side, side)\n",
    "    decoded = arr - 1\n",
    "    return \"\\n\".join(\" \".join(str(int(x)) for x in row) for row in decoded)\n",
    "\n",
    "\n",
    "def load_split(base_dir, split=\"train\"):\n",
    "    split_dir = Path(base_dir) / split\n",
    "    meta = json.loads((split_dir / \"dataset.json\").read_text())\n",
    "    inputs = np.load(split_dir / \"all__inputs.npy\")\n",
    "    labels = np.load(split_dir / \"all__labels.npy\")\n",
    "    puzzle_ids = np.load(split_dir / \"all__puzzle_identifiers.npy\")\n",
    "    return meta, inputs, labels, puzzle_ids\n",
    "\n",
    "\n",
    "meta_train, train_inputs, train_labels, train_puzzle_ids = load_split(DATA_ROOT, \"train\")\n",
    "meta_test, test_inputs, test_labels, test_puzzle_ids = load_split(DATA_ROOT, \"test\")\n",
    "dataset_meta = meta_train\n",
    "dataset_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9136e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyPuzzleDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, puzzle_ids):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.puzzle_ids = puzzle_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.inputs[idx], dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            torch.tensor(self.puzzle_ids[idx], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    inputs, labels, puzzle_ids = zip(*batch)\n",
    "    inputs = torch.stack(inputs, dim=0)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    puzzle_ids = torch.stack(puzzle_ids, dim=0)\n",
    "\n",
    "    return {\n",
    "        'inputs': inputs,\n",
    "        'labels': labels,\n",
    "        'puzzle_identifiers': puzzle_ids\n",
    "    }\n",
    "\n",
    "\n",
    "def make_loaders(batch_size=64):\n",
    "    train_ds = NumpyPuzzleDataset(train_inputs, train_labels, train_puzzle_ids)\n",
    "    test_ds = NumpyPuzzleDataset(test_inputs, test_labels, test_puzzle_ids)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "loaders = make_loaders(batch_size=64)\n",
    "{'train_batches': len(loaders[0]), 'test_batches': len(loaders[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c25824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_masked(logits, labels, ignore_index: int = -100, valid_mask=None):\n",
    "    # Compute CE in float32 and apply optional mask\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.to(torch.float32).view(-1, logits.shape[-1]),\n",
    "        labels.to(torch.long).view(-1),\n",
    "        ignore_index=ignore_index,\n",
    "        reduction='none',\n",
    "    ).view(labels.shape)\n",
    "    if valid_mask is None:\n",
    "        return loss\n",
    "    return loss * valid_mask\n",
    "\n",
    "def build_trm(meta, hidden_size=64, heads=4, halt_steps=2):\n",
    "    cfg = dict(\n",
    "        batch_size=32,\n",
    "        seq_len=meta['seq_len'],\n",
    "        puzzle_emb_ndim=0,\n",
    "        num_puzzle_identifiers=1,\n",
    "        vocab_size=meta['vocab_size'],\n",
    "        H_cycles=2,\n",
    "        L_cycles=2,\n",
    "        H_layers=0,\n",
    "        L_layers=1,\n",
    "        hidden_size=hidden_size,\n",
    "        expansion=2,\n",
    "        num_heads=heads,\n",
    "        pos_encodings='rope',\n",
    "        halt_max_steps=halt_steps,\n",
    "        halt_exploration_prob=0.05,\n",
    "        forward_dtype='float32',\n",
    "        mlp_t=True,\n",
    "        puzzle_emb_len=0,  # keep seq length aligned when puzzle_emb_ndim=0\n",
    "        no_ACT_continue=True,\n",
    "    )\n",
    "    model = TinyRecursiveReasoningModel_ACTV1(cfg)\n",
    "    return ACTLossHead(model, loss_type='softmax_cross_entropy_masked').to(device)\n",
    "\n",
    "\n",
    "def move_batch(batch):\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def move_carry_to_device(carry: TinyRecursiveReasoningModel_ACTV1Carry, device):\n",
    "    inner = TinyRecursiveReasoningModel_ACTV1InnerCarry(\n",
    "        z_H=carry.inner_carry.z_H.to(device),\n",
    "        z_L=carry.inner_carry.z_L.to(device),\n",
    ")\n",
    "    current_data = {k: v.to(device) for k, v in carry.current_data.items()}\n",
    "    return TinyRecursiveReasoningModel_ACTV1Carry(\n",
    "        inner_carry=inner,\n",
    "        steps=carry.steps.to(device),\n",
    "        halted=carry.halted.to(device),\n",
    "        current_data=current_data\n",
    ")\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    for batch in loader:\n",
    "        batch = move_batch(batch)\n",
    "        carry = model.initial_carry(batch)\n",
    "        carry = move_carry_to_device(carry, batch['inputs'].device)\n",
    "        carry, loss, metrics, outputs, _ = model(return_keys=['preds'], carry=carry, batch=batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs['preds']\n",
    "        total_correct += (preds == batch['labels']).sum().item()\n",
    "        total_tokens += batch['labels'].numel()\n",
    "    return {'loss': total_loss / max(len(loader), 1), 'token_acc': total_correct / max(total_tokens, 1)}\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = move_batch(batch)\n",
    "            carry = model.initial_carry(batch)\n",
    "            carry = move_carry_to_device(carry, batch['inputs'].device)\n",
    "            carry, loss, metrics, outputs, _ = model(return_keys=['preds'], carry=carry, batch=batch)\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs['preds']\n",
    "            total_correct += (preds == batch['labels']).sum().item()\n",
    "            total_tokens += batch['labels'].numel()\n",
    "    return {'loss': total_loss / max(len(loader), 1), 'token_acc': total_correct / max(total_tokens, 1)}\n",
    "\n",
    "\n",
    "def predict_batch(model, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = move_batch(batch)\n",
    "        carry = model.initial_carry(batch)\n",
    "        carry = move_carry_to_device(carry, batch['inputs'].device)\n",
    "        carry, _, _, outputs, _ = model(return_keys=['preds'], carry=carry, batch=batch)\n",
    "        return outputs['preds'].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = None\n",
    "training_logs = []\n",
    "\n",
    "print(f\"\\n=== Training sudoku (seq_len={dataset_meta['seq_len']}) ===\")\n",
    "model = build_trm(dataset_meta, hidden_size=96, heads=4, halt_steps=2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3, weight_decay=0.01)\n",
    "train_loader, test_loader = loaders\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_metrics = train_epoch(model, train_loader, optimizer)\n",
    "    val_metrics = evaluate(model, test_loader)\n",
    "    training_logs.append({\n",
    "        'task': 'sudoku9',\n",
    "        'epoch': epoch,\n",
    "        **{f\"train_{k}\": v for k, v in train_metrics.items()},\n",
    "        **{f\"val_{k}\": v for k, v in val_metrics.items()}\n",
    "    })\n",
    "    print(f\"epoch {epoch:02d} | train_loss {train_metrics['loss']:.3f} acc {train_metrics['token_acc']:.3f} | val_loss {val_metrics['loss']:.3f} acc {val_metrics['token_acc']:.3f}\")\n",
    "\n",
    "trained_model = model\n",
    "training_logs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_sample(meta, tensor_1d):\n",
    "    side = int(np.sqrt(meta['seq_len']))\n",
    "    return grid_to_text(tensor_1d, side=side)\n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "preds = predict_batch(trained_model, batch)\n",
    "rows = []\n",
    "\n",
    "for i in range(min(2, len(preds))):\n",
    "    rows.append({\n",
    "        'input': render_sample(dataset_meta, batch['inputs'][i].numpy()),\n",
    "        'target': render_sample(dataset_meta, batch['labels'][i].numpy()),\n",
    "        'prediction': render_sample(dataset_meta, preds[i])\n",
    "    })\n",
    "\n",
    "json.dumps(rows, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grid(text, side):\n",
    "    \"\"\"Parse an LLM text response into a side x side numpy grid of ints.\n",
    "    Accepts lines separated by newlines and cells separated by spaces or commas.\n",
    "    Returns None if parsing fails or the shape/value constraints are violated.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    lines = [ln.strip() for ln in text.strip().splitlines() if ln.strip()]\n",
    "    if len(lines) < 1:\n",
    "        return None\n",
    "    # Use first line that looks numeric-rich; otherwise concatenate\n",
    "    candidate_lines = lines\n",
    "    cells = []\n",
    "    for ln in candidate_lines:\n",
    "        parts = [p for p in ln.replace(',', ' ').split(' ') if p]\n",
    "        if len(parts) == side * side:\n",
    "            cells = parts\n",
    "            break\n",
    "        cells.extend(parts)\n",
    "    if len(cells) != side * side:\n",
    "        return None\n",
    "    try:\n",
    "        arr = np.array([int(p) for p in cells], dtype=np.int64)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    # Ensure values are 0-9 for Sudoku; allow 0 as blank\n",
    "    if not ((arr >= 0) & (arr <= 9)).all():\n",
    "        return None\n",
    "    try:\n",
    "        return arr.reshape(side, side)\n",
    "    except ValueError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(meta, grid_in, strategy='zero'):\n",
    "    side = int(np.sqrt(meta['seq_len']))\n",
    "    grid_txt = grid_to_text(grid_in, side=side)\n",
    "    header = f\"Fill the {side}x{side} Sudoku. Zeros/blanks mean empty cells. Return the completed grid with spaces separated.\"\n",
    "    prompt = f\"{header}\\nGrid:\\n{grid_txt}\\n\"\n",
    "    if strategy == 'cot':\n",
    "        prompt += \"Explain briefly then give the final grid.\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def evaluate_llm(meta, max_examples=5, strategies=(\"zero\", \"cot\")):\n",
    "    if ollama_client is None:\n",
    "        print(\"Ollama client not available; skipping LLM eval.\")\n",
    "        return []\n",
    "\n",
    "    side = int(np.sqrt(meta['seq_len']))\n",
    "    results = []\n",
    "    for idx, (inp, out) in enumerate(zip(test_inputs, test_labels)):\n",
    "        if idx >= max_examples:\n",
    "            break\n",
    "\n",
    "        for strat in strategies:\n",
    "            prompt = build_prompt(meta, inp, strategy=strat)\n",
    "            try:\n",
    "                resp = ollama_client.chat(model='ministral-3:3b', messages=[{'role': 'user', 'content': prompt}])\n",
    "                content = resp['message']['content'] if isinstance(resp, dict) else ''\n",
    "            except Exception as e:\n",
    "                content = f\"ERROR: {e}\"\n",
    "\n",
    "            parsed = parse_grid(content, side)\n",
    "            token_acc = None\n",
    "            parsed_tokens = None\n",
    "\n",
    "            if parsed is not None:\n",
    "                parsed_tokens = (parsed + 1).reshape(-1)\n",
    "                token_acc = float((parsed_tokens == out.reshape(-1)).mean())\n",
    "\n",
    "            results.append({\n",
    "                'task': 'sudoku9',\n",
    "                'example_id': idx,\n",
    "                'strategy': strat,\n",
    "                'prompt': prompt,\n",
    "                'raw_response': content,\n",
    "                'parsed': parsed.tolist() if parsed is not None else None,\n",
    "                'token_acc': token_acc\n",
    "            })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_results = evaluate_llm(dataset_meta, max_examples=1, strategies=(\"zero\", \"cot\"))\n",
    "llm_results[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d79029",
   "metadata": {},
   "outputs": [],
   "source": [
    "trm_eval = []\n",
    "metrics = evaluate(trained_model, test_loader)\n",
    "trm_eval.append({'task': 'sudoku9', 'model': 'trm', **metrics})\n",
    "\n",
    "trm_df = pd.DataFrame(trm_eval)\n",
    "llm_df = pd.DataFrame([r for r in llm_results if r.get('token_acc') is not None])\n",
    "\n",
    "if (not llm_df.empty) and {'task','strategy','token_acc'}.issubset(llm_df.columns):\n",
    "    llm_summary = llm_df.groupby(['task', 'strategy'])['token_acc'].mean().reset_index()\n",
    "else:\n",
    "    llm_summary = pd.DataFrame(columns=['task','strategy','token_acc'])\n",
    "\n",
    "trm_df, llm_summary.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
