{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438b2f1f",
   "metadata": {},
   "source": [
    "1. Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44017d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "from modules.device import best_device\n",
    "from datasets import load_dataset\n",
    "from data.dataset import build_tokenizer\n",
    "from modules.lstm import LSTMConfig, LSTMLanguageModel\n",
    "import torch\n",
    "from modules.transformer import TransformerConfig, TransformerLanguageModel\n",
    "from modules.training import train_streamed_lm\n",
    "from modules.inference import generate_text\n",
    "from modules.benchmark import measure_throughput\n",
    "from modules.eval import evaluate_on_hf_or_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535a7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: mps\n"
     ]
    }
   ],
   "source": [
    "print(\"Available device:\", best_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c15ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e15c778113b48f8909cfe970e0598bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer first (you can choose a different HF tokenizer if desired)\n",
    "tokenizer = build_tokenizer(\"gpt2\")\n",
    "\n",
    "# Load dataset (streaming to avoid full download)\n",
    "ds_stream = load_dataset(\"mikex86/stackoverflow-posts\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c4e686",
   "metadata": {},
   "source": [
    "2. Initialize LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59845b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM logits: torch.Size([2, 15, 50257]) loss: 10.825050354003906\n"
     ]
    }
   ],
   "source": [
    "# Initialize LSTM LM using HF tokenizer vocab size\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "lstm_cfg = LSTMConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=256,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "lstm = LSTMLanguageModel(lstm_cfg)\n",
    "\n",
    "# quick smoke test with tokenizer\n",
    "dummy_texts = [\n",
    "    \"Hello StackOverflow!\",\n",
    "    \"How do I calculate someone's age based on a DateTime type birthday?\",\n",
    "]\n",
    "enc = tokenizer(dummy_texts, return_tensors=\"pt\", padding=True)\n",
    "_dummy = enc[\"input_ids\"]\n",
    "logits, loss = lstm(_dummy, targets=_dummy)\n",
    "print(\"LSTM logits:\", logits.shape, \"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e73f40",
   "metadata": {},
   "source": [
    "3. Initialize Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb8f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer logits: torch.Size([2, 19, 50257]) loss: 10.836922645568848\n"
     ]
    }
   ],
   "source": [
    "# Initialize Transformer LM using HF tokenizer vocab size\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "tr_cfg = TransformerConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_dim=256,\n",
    "    n_heads=8,\n",
    "    n_layers=4,\n",
    "    ff_dim=1024,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=2048,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "trans = TransformerLanguageModel(tr_cfg)\n",
    "\n",
    "# quick smoke test with tokenizer\n",
    "dummy_texts = [\n",
    "    \"Given a DateTime representing a person's birthday, how do I calculate their age in years?\",\n",
    "    \"Calculate relative time in C#\",\n",
    "]\n",
    "enc = tokenizer(dummy_texts, return_tensors=\"pt\", padding=True)\n",
    "_dummy = enc[\"input_ids\"]\n",
    "logits, loss = trans(_dummy, targets=_dummy)\n",
    "print(\"Transformer logits:\", logits.shape, \"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2d6da",
   "metadata": {},
   "source": [
    "4. Training LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032b74e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: mps\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a044f5235ba44c8780ee29fa822665fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100 loss 9.7970\n",
      "step 200 loss 7.5953\n",
      "step 200 loss 7.5953\n",
      "step 300 loss 6.8217\n",
      "step 300 loss 6.8217\n",
      "step 400 loss 7.1827\n",
      "step 400 loss 7.1827\n",
      "step 500 loss 7.0969\n",
      "step 500 loss 7.0969\n",
      "step 600 loss 7.0844\n",
      "step 600 loss 7.0844\n",
      "step 700 loss 7.1052\n",
      "step 700 loss 7.1052\n",
      "step 800 loss 6.7242\n",
      "step 800 loss 6.7242\n",
      "step 900 loss 6.7620\n",
      "step 900 loss 6.7620\n",
      "step 1000 loss 7.0288\n",
      "step 1000 loss 7.0288\n",
      "step 1100 loss 7.0364\n",
      "step 1100 loss 7.0364\n",
      "step 1200 loss 7.0843\n",
      "step 1200 loss 7.0843\n",
      "step 1300 loss 6.9420\n",
      "step 1300 loss 6.9420\n",
      "step 1400 loss 6.9233\n",
      "step 1400 loss 6.9233\n",
      "step 1500 loss 6.0773\n",
      "step 1500 loss 6.0773\n",
      "step 1600 loss 6.4482\n",
      "step 1600 loss 6.4482\n",
      "step 1700 loss 6.3611\n",
      "step 1700 loss 6.3611\n",
      "step 1800 loss 6.1164\n",
      "step 1800 loss 6.1164\n",
      "step 1900 loss 6.1431\n",
      "step 1900 loss 6.1431\n",
      "step 2000 loss 6.1067\n",
      "step 2000 loss 6.1067\n",
      "step 2100 loss 6.0104\n",
      "step 2100 loss 6.0104\n",
      "step 2200 loss 5.8380\n",
      "step 2200 loss 5.8380\n",
      "step 2300 loss 5.7838\n",
      "step 2300 loss 5.7838\n",
      "step 2400 loss 6.0017\n",
      "step 2400 loss 6.0017\n",
      "step 2500 loss 5.7651\n",
      "step 2500 loss 5.7651\n",
      "step 2600 loss 5.7590\n",
      "step 2600 loss 5.7590\n",
      "step 2700 loss 5.9003\n",
      "step 2700 loss 5.9003\n",
      "step 2800 loss 5.6355\n",
      "step 2800 loss 5.6355\n",
      "step 2900 loss 5.4281\n",
      "step 2900 loss 5.4281\n",
      "step 3000 loss 5.5507\n",
      "step 3000 loss 5.5507\n",
      "step 3100 loss 5.5474\n",
      "step 3100 loss 5.5474\n",
      "step 3200 loss 5.4941\n",
      "step 3200 loss 5.4941\n",
      "step 3300 loss 5.7060\n",
      "step 3300 loss 5.7060\n",
      "step 3400 loss 5.3951\n",
      "step 3400 loss 5.3951\n",
      "step 3500 loss 5.2864\n",
      "step 3500 loss 5.2864\n",
      "step 3600 loss 5.3484\n",
      "step 3600 loss 5.3484\n",
      "step 3700 loss 5.2100\n",
      "step 3700 loss 5.2100\n",
      "step 3800 loss 5.5373\n",
      "step 3800 loss 5.5373\n",
      "step 3900 loss 5.3448\n",
      "step 3900 loss 5.3448\n",
      "step 4000 loss 5.5876\n",
      "step 4000 loss 5.5876\n",
      "step 4100 loss 5.1985\n",
      "step 4100 loss 5.1985\n",
      "step 4200 loss 5.1922\n",
      "step 4200 loss 5.1922\n",
      "step 4300 loss 5.1395\n",
      "step 4300 loss 5.1395\n",
      "step 4400 loss 5.2751\n",
      "step 4400 loss 5.2751\n",
      "step 4500 loss 4.9231\n",
      "step 4500 loss 4.9231\n",
      "step 4600 loss 5.3436\n",
      "step 4600 loss 5.3436\n",
      "step 4700 loss 5.5818\n",
      "step 4700 loss 5.5818\n",
      "step 4800 loss 5.0476\n",
      "step 4800 loss 5.0476\n",
      "step 4900 loss 4.9194\n",
      "step 4900 loss 4.9194\n",
      "step 5000 loss 5.0236\n",
      "step 5000 loss 5.0236\n",
      "step 5100 loss 5.3981\n",
      "step 5100 loss 5.3981\n",
      "step 5200 loss 4.2617\n",
      "step 5200 loss 4.2617\n",
      "step 5300 loss 5.1152\n",
      "step 5300 loss 5.1152\n",
      "step 5400 loss 5.1014\n",
      "step 5400 loss 5.1014\n",
      "step 5500 loss 4.7321\n",
      "step 5500 loss 4.7321\n",
      "step 5600 loss 4.8797\n",
      "step 5600 loss 4.8797\n",
      "step 5700 loss 5.0316\n",
      "step 5700 loss 5.0316\n",
      "step 5800 loss 5.1677\n",
      "step 5800 loss 5.1677\n",
      "step 5900 loss 4.9774\n",
      "step 5900 loss 4.9774\n",
      "step 6000 loss 5.4512\n",
      "step 6000 loss 5.4512\n",
      "step 6100 loss 5.1620\n",
      "step 6100 loss 5.1620\n",
      "step 6200 loss 4.7818\n",
      "step 6200 loss 4.7818\n",
      "step 6300 loss 5.0555\n",
      "step 6300 loss 5.0555\n",
      "step 6400 loss 4.8846\n",
      "step 6400 loss 4.8846\n",
      "step 6500 loss 5.1326\n",
      "step 6500 loss 5.1326\n",
      "step 6600 loss 5.1016\n",
      "step 6600 loss 5.1016\n",
      "step 6700 loss 5.1392\n",
      "step 6700 loss 5.1392\n",
      "step 6800 loss 5.0202\n",
      "step 6800 loss 5.0202\n",
      "step 6900 loss 5.0359\n",
      "step 6900 loss 5.0359\n",
      "step 7000 loss 4.9153\n",
      "step 7000 loss 4.9153\n",
      "step 7100 loss 4.8347\n",
      "step 7100 loss 4.8347\n",
      "step 7200 loss 4.8940\n",
      "step 7200 loss 4.8940\n",
      "step 7300 loss 4.8130\n",
      "step 7300 loss 4.8130\n",
      "step 7400 loss 4.9270\n",
      "step 7400 loss 4.9270\n",
      "step 7500 loss 4.6153\n",
      "step 7500 loss 4.6153\n",
      "step 7600 loss 4.9613\n",
      "step 7600 loss 4.9613\n",
      "step 7700 loss 4.6841\n",
      "step 7700 loss 4.6841\n",
      "step 7800 loss 4.7858\n",
      "step 7800 loss 4.7858\n",
      "step 7900 loss 4.6277\n",
      "step 7900 loss 4.6277\n",
      "step 8000 loss 4.7343\n",
      "step 8000 loss 4.7343\n",
      "step 8100 loss 4.1491\n",
      "step 8100 loss 4.1491\n",
      "step 8200 loss 4.6176\n",
      "step 8200 loss 4.6176\n",
      "step 8300 loss 4.9413\n",
      "step 8300 loss 4.9413\n",
      "step 8400 loss 4.9674\n",
      "step 8400 loss 4.9674\n",
      "step 8500 loss 4.8373\n",
      "step 8500 loss 4.8373\n",
      "step 8600 loss 4.7114\n",
      "step 8600 loss 4.7114\n",
      "step 8700 loss 4.9999\n",
      "step 8700 loss 4.9999\n",
      "step 8800 loss 4.6138\n",
      "step 8800 loss 4.6138\n",
      "step 8900 loss 4.7903\n",
      "step 8900 loss 4.7903\n",
      "step 9000 loss 4.6098\n",
      "step 9000 loss 4.6098\n",
      "step 9100 loss 4.7963\n",
      "step 9100 loss 4.7963\n",
      "step 9200 loss 4.9889\n",
      "step 9200 loss 4.9889\n",
      "step 9300 loss 4.4610\n",
      "step 9300 loss 4.4610\n",
      "step 9400 loss 4.8324\n",
      "step 9400 loss 4.8324\n",
      "step 9500 loss 4.2526\n",
      "step 9500 loss 4.2526\n",
      "step 9600 loss 4.6276\n",
      "step 9600 loss 4.6276\n",
      "step 9700 loss 4.5392\n",
      "step 9700 loss 4.5392\n",
      "step 9800 loss 4.6559\n",
      "step 9800 loss 4.6559\n",
      "step 9900 loss 4.4186\n",
      "step 9900 loss 4.4186\n",
      "step 10000 loss 4.6951\n",
      "step 10000 loss 4.6951\n",
      "Saved checkpoint: checkpoints/lstm/epoch_1.pt\n",
      "Epoch 2/5\n",
      "Saved checkpoint: checkpoints/lstm/epoch_1.pt\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 544393ad-48a6-4d2a-b87e-cd2381776dcc)')' thrown while requesting HEAD https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a7c62b763743799852b6f9e2869792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: e7dde5df-f2f2-4d24-8002-3e5f7f9b6100)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d2a6ce5d-34d2-449d-ba50-2dc3ed52cc4d)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d2a6ce5d-34d2-449d-ba50-2dc3ed52cc4d)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a412bd4a-2346-4f08-860f-c19d0c6e2775)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a412bd4a-2346-4f08-860f-c19d0c6e2775)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 250de84e-e751-428c-a0b5-9af244dd4ddf)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 250de84e-e751-428c-a0b5-9af244dd4ddf)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cfdab7b8-ff4b-4dba-958c-fefd74e58d02)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: cfdab7b8-ff4b-4dba-958c-fefd74e58d02)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: eaf218aa-6daa-41ee-9943-9c4f3178a094)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: eaf218aa-6daa-41ee-9943-9c4f3178a094)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f733b0a7-35ee-4cd2-ba81-b1a486517812)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f733b0a7-35ee-4cd2-ba81-b1a486517812)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10100 loss 4.3417\n",
      "step 10200 loss 5.0223\n",
      "step 10200 loss 5.0223\n",
      "step 10300 loss 4.4131\n",
      "step 10300 loss 4.4131\n",
      "step 10400 loss 4.9579\n",
      "step 10400 loss 4.9579\n",
      "step 10500 loss 4.9171\n",
      "step 10500 loss 4.9171\n",
      "step 10600 loss 4.6367\n",
      "step 10600 loss 4.6367\n",
      "step 10700 loss 4.4873\n",
      "step 10700 loss 4.4873\n",
      "step 10800 loss 4.1787\n",
      "step 10800 loss 4.1787\n",
      "step 10900 loss 4.4118\n",
      "step 10900 loss 4.4118\n",
      "step 11000 loss 4.8136\n",
      "step 11000 loss 4.8136\n",
      "step 11100 loss 4.7749\n",
      "step 11100 loss 4.7749\n",
      "step 11200 loss 4.7621\n",
      "step 11200 loss 4.7621\n",
      "step 11300 loss 4.7211\n",
      "step 11300 loss 4.7211\n",
      "step 11400 loss 4.5745\n",
      "step 11400 loss 4.5745\n",
      "step 11500 loss 4.1086\n",
      "step 11500 loss 4.1086\n",
      "step 11600 loss 4.2862\n",
      "step 11600 loss 4.2862\n",
      "step 11700 loss 4.5677\n",
      "step 11700 loss 4.5677\n",
      "step 11800 loss 4.4618\n",
      "step 11800 loss 4.4618\n",
      "step 11900 loss 4.6170\n",
      "step 11900 loss 4.6170\n",
      "step 12000 loss 4.7224\n",
      "step 12000 loss 4.7224\n",
      "step 12100 loss 4.7073\n",
      "step 12100 loss 4.7073\n",
      "step 12200 loss 4.5915\n",
      "step 12200 loss 4.5915\n",
      "step 12300 loss 4.6598\n",
      "step 12300 loss 4.6598\n",
      "step 12400 loss 4.7625\n",
      "step 12400 loss 4.7625\n",
      "step 12500 loss 4.5951\n",
      "step 12500 loss 4.5951\n",
      "step 12600 loss 4.7067\n",
      "step 12600 loss 4.7067\n",
      "step 12700 loss 4.9392\n",
      "step 12700 loss 4.9392\n",
      "step 12800 loss 4.6011\n",
      "step 12800 loss 4.6011\n",
      "step 12900 loss 4.5361\n",
      "step 12900 loss 4.5361\n",
      "step 13000 loss 4.5251\n",
      "step 13000 loss 4.5251\n",
      "step 13100 loss 4.6075\n",
      "step 13100 loss 4.6075\n",
      "step 13200 loss 4.6765\n",
      "step 13200 loss 4.6765\n",
      "step 13300 loss 4.7523\n",
      "step 13300 loss 4.7523\n",
      "step 13400 loss 4.6106\n",
      "step 13400 loss 4.6106\n",
      "step 13500 loss 4.5069\n",
      "step 13500 loss 4.5069\n",
      "step 13600 loss 4.5730\n",
      "step 13600 loss 4.5730\n",
      "step 13700 loss 4.3066\n",
      "step 13700 loss 4.3066\n",
      "step 13800 loss 4.7995\n",
      "step 13800 loss 4.7995\n",
      "step 13900 loss 4.5377\n",
      "step 13900 loss 4.5377\n",
      "step 14000 loss 4.9602\n",
      "step 14000 loss 4.9602\n",
      "step 14100 loss 4.5102\n",
      "step 14100 loss 4.5102\n",
      "step 14200 loss 4.5091\n",
      "step 14200 loss 4.5091\n",
      "step 14300 loss 4.5030\n",
      "step 14300 loss 4.5030\n",
      "step 14400 loss 4.5326\n",
      "step 14400 loss 4.5326\n",
      "step 14500 loss 4.1747\n",
      "step 14500 loss 4.1747\n",
      "step 14600 loss 4.6741\n",
      "step 14600 loss 4.6741\n",
      "step 14700 loss 4.9065\n",
      "step 14700 loss 4.9065\n",
      "step 14800 loss 4.3623\n",
      "step 14800 loss 4.3623\n",
      "step 14900 loss 4.2319\n",
      "step 14900 loss 4.2319\n",
      "step 15000 loss 4.3572\n",
      "step 15000 loss 4.3572\n",
      "step 15100 loss 4.7572\n",
      "step 15100 loss 4.7572\n",
      "step 15200 loss 3.7778\n",
      "step 15200 loss 3.7778\n",
      "step 15300 loss 4.4939\n",
      "step 15300 loss 4.4939\n",
      "step 15400 loss 4.5298\n",
      "step 15400 loss 4.5298\n",
      "step 15500 loss 4.1306\n",
      "step 15500 loss 4.1306\n",
      "step 15600 loss 4.3160\n",
      "step 15600 loss 4.3160\n",
      "step 15700 loss 4.4593\n",
      "step 15700 loss 4.4593\n",
      "step 15800 loss 4.6308\n",
      "step 15800 loss 4.6308\n",
      "step 15900 loss 4.4546\n",
      "step 15900 loss 4.4546\n",
      "step 16000 loss 4.8933\n",
      "step 16000 loss 4.8933\n",
      "step 16100 loss 4.6413\n",
      "step 16100 loss 4.6413\n",
      "step 16200 loss 4.2462\n",
      "step 16200 loss 4.2462\n",
      "step 16300 loss 4.5626\n",
      "step 16300 loss 4.5626\n",
      "step 16400 loss 4.3229\n",
      "step 16400 loss 4.3229\n",
      "step 16500 loss 4.6106\n",
      "step 16500 loss 4.6106\n",
      "step 16600 loss 4.5842\n",
      "step 16600 loss 4.5842\n",
      "step 16700 loss 4.6330\n",
      "step 16700 loss 4.6330\n",
      "step 16800 loss 4.4037\n",
      "step 16800 loss 4.4037\n",
      "step 16900 loss 4.5257\n",
      "step 16900 loss 4.5257\n",
      "step 17000 loss 4.3973\n",
      "step 17000 loss 4.3973\n",
      "step 17100 loss 4.3414\n",
      "step 17100 loss 4.3414\n",
      "step 17200 loss 4.3590\n",
      "step 17200 loss 4.3590\n",
      "step 17300 loss 4.3200\n",
      "step 17300 loss 4.3200\n",
      "step 17400 loss 4.4948\n",
      "step 17400 loss 4.4948\n",
      "step 17500 loss 4.1694\n",
      "step 17500 loss 4.1694\n",
      "step 17600 loss 4.4815\n",
      "step 17600 loss 4.4815\n",
      "step 17700 loss 4.1948\n",
      "step 17700 loss 4.1948\n",
      "step 17800 loss 4.3652\n",
      "step 17800 loss 4.3652\n",
      "step 17900 loss 4.1432\n",
      "step 17900 loss 4.1432\n",
      "step 18000 loss 4.3155\n",
      "step 18000 loss 4.3155\n",
      "step 18100 loss 3.7204\n",
      "step 18100 loss 3.7204\n",
      "step 18200 loss 4.1756\n",
      "step 18200 loss 4.1756\n",
      "step 18300 loss 4.4815\n",
      "step 18300 loss 4.4815\n",
      "step 18400 loss 4.5213\n",
      "step 18400 loss 4.5213\n",
      "step 18500 loss 4.4953\n",
      "step 18500 loss 4.4953\n",
      "step 18600 loss 4.3160\n",
      "step 18600 loss 4.3160\n",
      "step 18700 loss 4.5622\n",
      "step 18700 loss 4.5622\n",
      "step 18800 loss 4.1748\n",
      "step 18800 loss 4.1748\n",
      "step 18900 loss 4.3796\n",
      "step 18900 loss 4.3796\n",
      "step 19000 loss 4.2038\n",
      "step 19000 loss 4.2038\n",
      "step 19100 loss 4.4076\n",
      "step 19100 loss 4.4076\n",
      "step 19200 loss 4.5948\n",
      "step 19200 loss 4.5948\n",
      "step 19300 loss 4.0755\n",
      "step 19300 loss 4.0755\n",
      "step 19400 loss 4.4662\n",
      "step 19400 loss 4.4662\n",
      "step 19500 loss 3.8886\n",
      "step 19500 loss 3.8886\n",
      "step 19600 loss 4.1686\n",
      "step 19600 loss 4.1686\n",
      "step 19700 loss 4.1413\n",
      "step 19700 loss 4.1413\n",
      "step 19800 loss 4.2809\n",
      "step 19800 loss 4.2809\n",
      "step 19900 loss 4.1100\n",
      "step 19900 loss 4.1100\n",
      "step 20000 loss 4.3709\n",
      "step 20000 loss 4.3709\n",
      "Saved checkpoint: checkpoints/lstm/epoch_2.pt\n",
      "Epoch 3/5\n",
      "Saved checkpoint: checkpoints/lstm/epoch_2.pt\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17d863d31184f3e8c12354b3035eb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20100 loss 3.8640\n",
      "step 20200 loss 4.5480\n",
      "step 20200 loss 4.5480\n",
      "step 20300 loss 4.1126\n",
      "step 20300 loss 4.1126\n",
      "step 20400 loss 4.6029\n",
      "step 20400 loss 4.6029\n",
      "step 20500 loss 4.5897\n",
      "step 20500 loss 4.5897\n",
      "step 20600 loss 4.2783\n",
      "step 20600 loss 4.2783\n",
      "step 20700 loss 4.0984\n",
      "step 20700 loss 4.0984\n",
      "step 20800 loss 3.8557\n",
      "step 20800 loss 3.8557\n",
      "step 20900 loss 4.1106\n",
      "step 20900 loss 4.1106\n",
      "step 21000 loss 4.4771\n",
      "step 21000 loss 4.4771\n",
      "step 21100 loss 4.4711\n",
      "step 21100 loss 4.4711\n",
      "step 21200 loss 4.3898\n",
      "step 21200 loss 4.3898\n",
      "step 21300 loss 4.3745\n",
      "step 21300 loss 4.3745\n",
      "step 21400 loss 4.2211\n",
      "step 21400 loss 4.2211\n",
      "step 21500 loss 3.7827\n",
      "step 21500 loss 3.7827\n",
      "step 21600 loss 3.9383\n",
      "step 21600 loss 3.9383\n",
      "step 21700 loss 4.2640\n",
      "step 21700 loss 4.2640\n",
      "step 21800 loss 4.1424\n",
      "step 21800 loss 4.1424\n",
      "step 21900 loss 4.3557\n",
      "step 21900 loss 4.3557\n",
      "step 22000 loss 4.4400\n",
      "step 22000 loss 4.4400\n",
      "step 22100 loss 4.4084\n",
      "step 22100 loss 4.4084\n",
      "step 22200 loss 4.2986\n",
      "step 22200 loss 4.2986\n",
      "step 22300 loss 4.3712\n",
      "step 22300 loss 4.3712\n",
      "step 22400 loss 4.4420\n",
      "step 22400 loss 4.4420\n",
      "step 22500 loss 4.2082\n",
      "step 22500 loss 4.2082\n",
      "step 22600 loss 4.4133\n",
      "step 22600 loss 4.4133\n",
      "step 22700 loss 4.6449\n",
      "step 22700 loss 4.6449\n",
      "step 22800 loss 4.2984\n",
      "step 22800 loss 4.2984\n",
      "step 22900 loss 4.2751\n",
      "step 22900 loss 4.2751\n",
      "step 23000 loss 4.2058\n",
      "step 23000 loss 4.2058\n",
      "step 23100 loss 4.3020\n",
      "step 23100 loss 4.3020\n",
      "step 23200 loss 4.4301\n",
      "step 23200 loss 4.4301\n",
      "step 23300 loss 4.4337\n",
      "step 23300 loss 4.4337\n",
      "step 23400 loss 4.3684\n",
      "step 23400 loss 4.3684\n",
      "step 23500 loss 4.2831\n",
      "step 23500 loss 4.2831\n",
      "step 23600 loss 4.2748\n",
      "step 23600 loss 4.2748\n",
      "step 23700 loss 3.9901\n",
      "step 23700 loss 3.9901\n",
      "step 23800 loss 4.5547\n",
      "step 23800 loss 4.5547\n",
      "step 23900 loss 4.2453\n",
      "step 23900 loss 4.2453\n",
      "step 24000 loss 4.6980\n",
      "step 24000 loss 4.6980\n",
      "step 24100 loss 4.3070\n",
      "step 24100 loss 4.3070\n",
      "step 24200 loss 4.2992\n",
      "step 24200 loss 4.2992\n",
      "step 24300 loss 4.2575\n",
      "step 24300 loss 4.2575\n",
      "step 24400 loss 4.2560\n",
      "step 24400 loss 4.2560\n",
      "step 24500 loss 3.8795\n",
      "step 24500 loss 3.8795\n",
      "step 24600 loss 4.4259\n",
      "step 24600 loss 4.4259\n",
      "step 24700 loss 4.5801\n",
      "step 24700 loss 4.5801\n",
      "step 24800 loss 4.0786\n",
      "step 24800 loss 4.0786\n",
      "step 24900 loss 3.9912\n",
      "step 24900 loss 3.9912\n",
      "step 25000 loss 4.1063\n",
      "step 25000 loss 4.1063\n",
      "step 25100 loss 4.5184\n",
      "step 25100 loss 4.5184\n",
      "step 25200 loss 3.5924\n",
      "step 25200 loss 3.5924\n",
      "step 25300 loss 4.2315\n",
      "step 25300 loss 4.2315\n",
      "step 25400 loss 4.2827\n",
      "step 25400 loss 4.2827\n",
      "step 25500 loss 3.8957\n",
      "step 25500 loss 3.8957\n",
      "step 25600 loss 4.0488\n",
      "step 25600 loss 4.0488\n",
      "step 25700 loss 4.2165\n",
      "step 25700 loss 4.2165\n",
      "step 25800 loss 4.3726\n",
      "step 25800 loss 4.3726\n",
      "step 25900 loss 4.2110\n",
      "step 25900 loss 4.2110\n",
      "step 26000 loss 4.6460\n",
      "step 26000 loss 4.6460\n",
      "step 26100 loss 4.4072\n",
      "step 26100 loss 4.4072\n",
      "step 26200 loss 3.9957\n",
      "step 26200 loss 3.9957\n",
      "step 26300 loss 4.3436\n",
      "step 26300 loss 4.3436\n",
      "step 26400 loss 4.0846\n",
      "step 26400 loss 4.0846\n",
      "step 26500 loss 4.3588\n",
      "step 26500 loss 4.3588\n",
      "step 26600 loss 4.3248\n",
      "step 26600 loss 4.3248\n",
      "step 26700 loss 4.3785\n",
      "step 26700 loss 4.3785\n",
      "step 26800 loss 4.1535\n",
      "step 26800 loss 4.1535\n",
      "step 26900 loss 4.3070\n",
      "step 26900 loss 4.3070\n",
      "step 27000 loss 4.1797\n",
      "step 27000 loss 4.1797\n",
      "step 27100 loss 4.1257\n",
      "step 27100 loss 4.1257\n",
      "step 27200 loss 4.1363\n",
      "step 27200 loss 4.1363\n",
      "step 27300 loss 4.1172\n",
      "step 27300 loss 4.1172\n",
      "step 27400 loss 4.3110\n",
      "step 27400 loss 4.3110\n",
      "step 27500 loss 3.9483\n",
      "step 27500 loss 3.9483\n",
      "step 27600 loss 4.2725\n",
      "step 27600 loss 4.2725\n",
      "step 27700 loss 3.9506\n",
      "step 27700 loss 3.9506\n",
      "step 27800 loss 4.1498\n",
      "step 27800 loss 4.1498\n",
      "step 27900 loss 3.9360\n",
      "step 27900 loss 3.9360\n",
      "step 28000 loss 4.1173\n",
      "step 28000 loss 4.1173\n",
      "step 28100 loss 3.5139\n",
      "step 28100 loss 3.5139\n",
      "step 28200 loss 3.9676\n",
      "step 28200 loss 3.9676\n",
      "step 28300 loss 4.2613\n",
      "step 28300 loss 4.2613\n",
      "step 28400 loss 4.3033\n",
      "step 28400 loss 4.3033\n",
      "step 28500 loss 4.2993\n",
      "step 28500 loss 4.2993\n",
      "step 28600 loss 4.1399\n",
      "step 28600 loss 4.1399\n",
      "step 28700 loss 4.3341\n",
      "step 28700 loss 4.3341\n",
      "step 28800 loss 3.9558\n",
      "step 28800 loss 3.9558\n",
      "step 28900 loss 4.1814\n",
      "step 28900 loss 4.1814\n",
      "step 29000 loss 3.9919\n",
      "step 29000 loss 3.9919\n",
      "step 29100 loss 4.2190\n",
      "step 29100 loss 4.2190\n",
      "step 29200 loss 4.4009\n",
      "step 29200 loss 4.4009\n",
      "step 29300 loss 3.8786\n",
      "step 29300 loss 3.8786\n",
      "step 29400 loss 4.2758\n",
      "step 29400 loss 4.2758\n",
      "step 29500 loss 3.6993\n",
      "step 29500 loss 3.6993\n",
      "step 29600 loss 3.9171\n",
      "step 29600 loss 3.9171\n",
      "step 29700 loss 3.9120\n",
      "step 29700 loss 3.9120\n",
      "step 29800 loss 4.0835\n",
      "step 29800 loss 4.0835\n",
      "step 29900 loss 3.9258\n",
      "step 29900 loss 3.9258\n",
      "step 30000 loss 4.2114\n",
      "step 30000 loss 4.2114\n",
      "Saved checkpoint: checkpoints/lstm/epoch_3.pt\n",
      "Epoch 4/5\n",
      "Saved checkpoint: checkpoints/lstm/epoch_3.pt\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3057e51837741e9b59d665c482cd77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30100 loss 3.6207\n",
      "step 30200 loss 4.3141\n",
      "step 30200 loss 4.3141\n",
      "step 30300 loss 3.9717\n",
      "step 30300 loss 3.9717\n",
      "step 30400 loss 4.4082\n",
      "step 30400 loss 4.4082\n",
      "step 30500 loss 4.3588\n",
      "step 30500 loss 4.3588\n",
      "step 30600 loss 4.0782\n",
      "step 30600 loss 4.0782\n",
      "step 30700 loss 3.8787\n",
      "step 30700 loss 3.8787\n",
      "step 30800 loss 3.6992\n",
      "step 30800 loss 3.6992\n",
      "step 30900 loss 3.9431\n",
      "step 30900 loss 3.9431\n",
      "step 31000 loss 4.2925\n",
      "step 31000 loss 4.2925\n",
      "step 31100 loss 4.2876\n",
      "step 31100 loss 4.2876\n",
      "step 31200 loss 4.2356\n",
      "step 31200 loss 4.2356\n",
      "step 31300 loss 4.1796\n",
      "step 31300 loss 4.1796\n",
      "step 31400 loss 4.0322\n",
      "step 31400 loss 4.0322\n",
      "step 31500 loss 3.5973\n",
      "step 31500 loss 3.5973\n",
      "step 31600 loss 3.7376\n",
      "step 31600 loss 3.7376\n",
      "step 31700 loss 4.0778\n",
      "step 31700 loss 4.0778\n",
      "step 31800 loss 3.9697\n",
      "step 31800 loss 3.9697\n",
      "step 31900 loss 4.1836\n",
      "step 31900 loss 4.1836\n",
      "step 32000 loss 4.2872\n",
      "step 32000 loss 4.2872\n",
      "step 32100 loss 4.2368\n",
      "step 32100 loss 4.2368\n",
      "step 32200 loss 4.1137\n",
      "step 32200 loss 4.1137\n",
      "step 32300 loss 4.2061\n",
      "step 32300 loss 4.2061\n",
      "step 32400 loss 4.2251\n",
      "step 32400 loss 4.2251\n",
      "step 32500 loss 3.9901\n",
      "step 32500 loss 3.9901\n",
      "step 32600 loss 4.2262\n",
      "step 32600 loss 4.2262\n",
      "step 32700 loss 4.4454\n",
      "step 32700 loss 4.4454\n",
      "step 32800 loss 4.1523\n",
      "step 32800 loss 4.1523\n",
      "step 32900 loss 4.1127\n",
      "step 32900 loss 4.1127\n",
      "step 33000 loss 4.0397\n",
      "step 33000 loss 4.0397\n",
      "step 33100 loss 4.1496\n",
      "step 33100 loss 4.1496\n",
      "step 33200 loss 4.2596\n",
      "step 33200 loss 4.2596\n",
      "step 33300 loss 4.2659\n",
      "step 33300 loss 4.2659\n",
      "step 33400 loss 4.2327\n",
      "step 33400 loss 4.2327\n",
      "step 33500 loss 4.1439\n",
      "step 33500 loss 4.1439\n",
      "step 33600 loss 4.1034\n",
      "step 33600 loss 4.1034\n",
      "step 33700 loss 3.8022\n",
      "step 33700 loss 3.8022\n",
      "step 33800 loss 4.3996\n",
      "step 33800 loss 4.3996\n",
      "step 33900 loss 4.0916\n",
      "step 33900 loss 4.0916\n",
      "step 34000 loss 4.5367\n",
      "step 34000 loss 4.5367\n",
      "step 34100 loss 4.1672\n",
      "step 34100 loss 4.1672\n",
      "step 34200 loss 4.1418\n",
      "step 34200 loss 4.1418\n",
      "step 34300 loss 4.0907\n",
      "step 34300 loss 4.0907\n",
      "step 34400 loss 4.0795\n",
      "step 34400 loss 4.0795\n",
      "step 34500 loss 3.7141\n",
      "step 34500 loss 3.7141\n",
      "step 34600 loss 4.2776\n",
      "step 34600 loss 4.2776\n",
      "step 34700 loss 4.3979\n",
      "step 34700 loss 4.3979\n",
      "step 34800 loss 3.9089\n",
      "step 34800 loss 3.9089\n",
      "step 34900 loss 3.8474\n",
      "step 34900 loss 3.8474\n",
      "step 35000 loss 3.9626\n",
      "step 35000 loss 3.9626\n",
      "step 35100 loss 4.3745\n",
      "step 35100 loss 4.3745\n",
      "step 35200 loss 3.4652\n",
      "step 35200 loss 3.4652\n",
      "step 35300 loss 4.0729\n",
      "step 35300 loss 4.0729\n",
      "step 35400 loss 4.1241\n",
      "step 35400 loss 4.1241\n",
      "step 35500 loss 3.7366\n",
      "step 35500 loss 3.7366\n",
      "step 35600 loss 3.9112\n",
      "step 35600 loss 3.9112\n",
      "step 35700 loss 4.0684\n",
      "step 35700 loss 4.0684\n",
      "step 35800 loss 4.2294\n",
      "step 35800 loss 4.2294\n",
      "step 35900 loss 4.0627\n",
      "step 35900 loss 4.0627\n",
      "step 36000 loss 4.4755\n",
      "step 36000 loss 4.4755\n",
      "step 36100 loss 4.2444\n",
      "step 36100 loss 4.2444\n",
      "step 36200 loss 3.8441\n",
      "step 36200 loss 3.8441\n",
      "step 36300 loss 4.2301\n",
      "step 36300 loss 4.2301\n",
      "step 36400 loss 3.9139\n",
      "step 36400 loss 3.9139\n",
      "step 36500 loss 4.2075\n",
      "step 36500 loss 4.2075\n",
      "step 36600 loss 4.1915\n",
      "step 36600 loss 4.1915\n",
      "step 36700 loss 4.2625\n",
      "step 36700 loss 4.2625\n",
      "step 36800 loss 3.9829\n",
      "step 36800 loss 3.9829\n",
      "step 36900 loss 4.1524\n",
      "step 36900 loss 4.1524\n",
      "step 37000 loss 4.0507\n",
      "step 37000 loss 4.0507\n",
      "step 37100 loss 3.9777\n",
      "step 37100 loss 3.9777\n",
      "step 37200 loss 4.0027\n",
      "step 37200 loss 4.0027\n",
      "step 37300 loss 3.9771\n",
      "step 37300 loss 3.9771\n",
      "step 37400 loss 4.1959\n",
      "step 37400 loss 4.1959\n",
      "step 37500 loss 3.8116\n",
      "step 37500 loss 3.8116\n",
      "step 37600 loss 4.1251\n",
      "step 37600 loss 4.1251\n",
      "step 37700 loss 3.7914\n",
      "step 37700 loss 3.7914\n",
      "step 37800 loss 4.0086\n",
      "step 37800 loss 4.0086\n",
      "step 37900 loss 3.7920\n",
      "step 37900 loss 3.7920\n",
      "step 38000 loss 4.0159\n",
      "step 38000 loss 4.0159\n",
      "step 38100 loss 3.3824\n",
      "step 38100 loss 3.3824\n",
      "step 38200 loss 3.8386\n",
      "step 38200 loss 3.8386\n",
      "step 38300 loss 4.1263\n",
      "step 38300 loss 4.1263\n",
      "step 38400 loss 4.1514\n",
      "step 38400 loss 4.1514\n",
      "step 38500 loss 4.1844\n",
      "step 38500 loss 4.1844\n",
      "step 38600 loss 4.0218\n",
      "step 38600 loss 4.0218\n",
      "step 38700 loss 4.1948\n",
      "step 38700 loss 4.1948\n",
      "step 38800 loss 3.8205\n",
      "step 38800 loss 3.8205\n",
      "step 38900 loss 4.0369\n",
      "step 38900 loss 4.0369\n",
      "step 39000 loss 3.8595\n",
      "step 39000 loss 3.8595\n",
      "step 39100 loss 4.0908\n",
      "step 39100 loss 4.0908\n",
      "step 39200 loss 4.2883\n",
      "step 39200 loss 4.2883\n",
      "step 39300 loss 3.7458\n",
      "step 39300 loss 3.7458\n",
      "step 39400 loss 4.1610\n",
      "step 39400 loss 4.1610\n",
      "step 39500 loss 3.5774\n",
      "step 39500 loss 3.5774\n",
      "step 39600 loss 3.7570\n",
      "step 39600 loss 3.7570\n",
      "step 39700 loss 3.7753\n",
      "step 39700 loss 3.7753\n",
      "step 39800 loss 3.9688\n",
      "step 39800 loss 3.9688\n",
      "step 39900 loss 3.8052\n",
      "step 39900 loss 3.8052\n",
      "step 40000 loss 4.1053\n",
      "step 40000 loss 4.1053\n",
      "Saved checkpoint: checkpoints/lstm/epoch_4.pt\n",
      "Epoch 5/5\n",
      "Saved checkpoint: checkpoints/lstm/epoch_4.pt\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e6e3592ac148f9a346442406649630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 40100 loss 3.4780\n",
      "step 40200 loss 4.1811\n",
      "step 40200 loss 4.1811\n",
      "step 40300 loss 3.8793\n",
      "step 40300 loss 3.8793\n",
      "step 40400 loss 4.2770\n",
      "step 40400 loss 4.2770\n",
      "step 40500 loss 4.2328\n",
      "step 40500 loss 4.2328\n",
      "step 40600 loss 3.9499\n",
      "step 40600 loss 3.9499\n",
      "step 40700 loss 3.7529\n",
      "step 40700 loss 3.7529\n",
      "step 40800 loss 3.5976\n",
      "step 40800 loss 3.5976\n",
      "step 40900 loss 3.8396\n",
      "step 40900 loss 3.8396\n",
      "step 41000 loss 4.1874\n",
      "step 41000 loss 4.1874\n",
      "step 41100 loss 4.1573\n",
      "step 41100 loss 4.1573\n",
      "step 41200 loss 4.1213\n",
      "step 41200 loss 4.1213\n",
      "step 41300 loss 4.0728\n",
      "step 41300 loss 4.0728\n",
      "step 41400 loss 3.9332\n",
      "step 41400 loss 3.9332\n",
      "step 41500 loss 3.4960\n",
      "step 41500 loss 3.4960\n",
      "step 41600 loss 3.6121\n",
      "step 41600 loss 3.6121\n",
      "step 41700 loss 3.9728\n",
      "step 41700 loss 3.9728\n",
      "step 41800 loss 3.8773\n",
      "step 41800 loss 3.8773\n",
      "step 41900 loss 4.0792\n",
      "step 41900 loss 4.0792\n",
      "step 42000 loss 4.1740\n",
      "step 42000 loss 4.1740\n",
      "step 42100 loss 4.1563\n",
      "step 42100 loss 4.1563\n",
      "step 42200 loss 4.0042\n",
      "step 42200 loss 4.0042\n",
      "step 42300 loss 4.1195\n",
      "step 42300 loss 4.1195\n",
      "step 42400 loss 4.0986\n",
      "step 42400 loss 4.0986\n",
      "step 42500 loss 3.8757\n",
      "step 42500 loss 3.8757\n",
      "step 42600 loss 4.0971\n",
      "step 42600 loss 4.0971\n",
      "step 42700 loss 4.3402\n",
      "step 42700 loss 4.3402\n",
      "step 42800 loss 4.0454\n",
      "step 42800 loss 4.0454\n",
      "step 42900 loss 4.0170\n",
      "step 42900 loss 4.0170\n",
      "step 43000 loss 3.9085\n",
      "step 43000 loss 3.9085\n",
      "step 43100 loss 4.0348\n",
      "step 43100 loss 4.0348\n",
      "step 43200 loss 4.1600\n",
      "step 43200 loss 4.1600\n",
      "step 43300 loss 4.1113\n",
      "step 43300 loss 4.1113\n",
      "step 43400 loss 4.1371\n",
      "step 43400 loss 4.1371\n",
      "step 43500 loss 4.0428\n",
      "step 43500 loss 4.0428\n",
      "step 43600 loss 3.9623\n",
      "step 43600 loss 3.9623\n",
      "step 43700 loss 3.6513\n",
      "step 43700 loss 3.6513\n",
      "step 43800 loss 4.3124\n",
      "step 43800 loss 4.3124\n",
      "step 43900 loss 3.9719\n",
      "step 43900 loss 3.9719\n",
      "step 44000 loss 4.4466\n",
      "step 44000 loss 4.4466\n",
      "step 44100 loss 4.0683\n",
      "step 44100 loss 4.0683\n",
      "step 44200 loss 4.0397\n",
      "step 44200 loss 4.0397\n",
      "step 44300 loss 4.0031\n",
      "step 44300 loss 4.0031\n",
      "step 44400 loss 3.9682\n",
      "step 44400 loss 3.9682\n",
      "step 44500 loss 3.6187\n",
      "step 44500 loss 3.6187\n",
      "step 44600 loss 4.1428\n",
      "step 44600 loss 4.1428\n",
      "step 44700 loss 4.2611\n",
      "step 44700 loss 4.2611\n",
      "step 44800 loss 3.8166\n",
      "step 44800 loss 3.8166\n",
      "step 44900 loss 3.7374\n",
      "step 44900 loss 3.7374\n",
      "step 45000 loss 3.8668\n",
      "step 45000 loss 3.8668\n",
      "step 45100 loss 4.2832\n",
      "step 45100 loss 4.2832\n",
      "step 45200 loss 3.3740\n",
      "step 45200 loss 3.3740\n",
      "step 45300 loss 3.9741\n",
      "step 45300 loss 3.9741\n",
      "step 45400 loss 4.0228\n",
      "step 45400 loss 4.0228\n",
      "step 45500 loss 3.6277\n",
      "step 45500 loss 3.6277\n",
      "step 45600 loss 3.7961\n",
      "step 45600 loss 3.7961\n",
      "step 45700 loss 3.9575\n",
      "step 45700 loss 3.9575\n",
      "step 45800 loss 4.1145\n",
      "step 45800 loss 4.1145\n",
      "step 45900 loss 3.9503\n",
      "step 45900 loss 3.9503\n",
      "step 46000 loss 4.3976\n",
      "step 46000 loss 4.3976\n",
      "step 46100 loss 4.1541\n",
      "step 46100 loss 4.1541\n",
      "step 46200 loss 3.7359\n",
      "step 46200 loss 3.7359\n",
      "step 46300 loss 4.1285\n",
      "step 46300 loss 4.1285\n",
      "step 46400 loss 3.7997\n",
      "step 46400 loss 3.7997\n",
      "step 46500 loss 4.1282\n",
      "step 46500 loss 4.1282\n",
      "step 46600 loss 4.0763\n",
      "step 46600 loss 4.0763\n",
      "step 46700 loss 4.1861\n",
      "step 46700 loss 4.1861\n",
      "step 46800 loss 3.8551\n",
      "step 46800 loss 3.8551\n",
      "step 46900 loss 4.0560\n",
      "step 46900 loss 4.0560\n",
      "step 47000 loss 3.9751\n",
      "step 47000 loss 3.9751\n",
      "step 47100 loss 3.8815\n",
      "step 47100 loss 3.8815\n",
      "step 47200 loss 3.9016\n",
      "step 47200 loss 3.9016\n",
      "step 47300 loss 3.8926\n",
      "step 47300 loss 3.8926\n",
      "step 47400 loss 4.0710\n",
      "step 47400 loss 4.0710\n",
      "step 47500 loss 3.7137\n",
      "step 47500 loss 3.7137\n",
      "step 47600 loss 4.0362\n",
      "step 47600 loss 4.0362\n",
      "step 47700 loss 3.6972\n",
      "step 47700 loss 3.6972\n",
      "step 47800 loss 3.9297\n",
      "step 47800 loss 3.9297\n",
      "step 47900 loss 3.6877\n",
      "step 47900 loss 3.6877\n",
      "step 48000 loss 3.9233\n",
      "step 48000 loss 3.9233\n",
      "step 48100 loss 3.3102\n",
      "step 48100 loss 3.3102\n",
      "step 48200 loss 3.7365\n",
      "step 48200 loss 3.7365\n",
      "step 48300 loss 4.0351\n",
      "step 48300 loss 4.0351\n",
      "step 48400 loss 4.0535\n",
      "step 48400 loss 4.0535\n",
      "step 48500 loss 4.0982\n",
      "step 48500 loss 4.0982\n",
      "step 48600 loss 3.9348\n",
      "step 48600 loss 3.9348\n",
      "step 48700 loss 4.1208\n",
      "step 48700 loss 4.1208\n",
      "step 48800 loss 3.7190\n",
      "step 48800 loss 3.7190\n",
      "step 48900 loss 3.9446\n",
      "step 48900 loss 3.9446\n",
      "step 49000 loss 3.7791\n",
      "step 49000 loss 3.7791\n",
      "step 49100 loss 3.9823\n",
      "step 49100 loss 3.9823\n",
      "step 49200 loss 4.2026\n",
      "step 49200 loss 4.2026\n",
      "step 49300 loss 3.6724\n",
      "step 49300 loss 3.6724\n",
      "step 49400 loss 4.0704\n",
      "step 49400 loss 4.0704\n",
      "step 49500 loss 3.5028\n",
      "step 49500 loss 3.5028\n",
      "step 49600 loss 3.6490\n",
      "step 49600 loss 3.6490\n",
      "step 49700 loss 3.6919\n",
      "step 49700 loss 3.6919\n",
      "step 49800 loss 3.8712\n",
      "step 49800 loss 3.8712\n",
      "step 49900 loss 3.7273\n",
      "step 49900 loss 3.7273\n",
      "step 50000 loss 4.0280\n",
      "step 50000 loss 4.0280\n",
      "Saved checkpoint: checkpoints/lstm/epoch_5.pt\n",
      "Saved checkpoint: checkpoints/lstm/epoch_5.pt\n",
      "Training complete. Saved to: outputs/lstm/final.pt\n",
      "Training complete. Saved to: outputs/lstm/final.pt\n"
     ]
    }
   ],
   "source": [
    "# Full training (LSTM) using shared utilities\n",
    "lstm_final = train_streamed_lm(\n",
    "    model=lstm,\n",
    "    tokenizer=tokenizer,\n",
    "    config=lstm_cfg.__dict__,\n",
    "    ckpt_dir=\"checkpoints/lstm\",\n",
    "    final_dir=\"outputs/lstm\",\n",
    "    batch_size=16,\n",
    "    max_length=256,\n",
    "    steps_per_epoch=10000,\n",
    "    num_epochs=5,\n",
    "    save_every=1,\n",
    "    lr=3e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32692c",
   "metadata": {},
   "source": [
    "5. Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e03bde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: mps\n",
      "Epoch 1/5\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19eab5c5d5864c929346f9eb6ceaf16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100 loss 10.2686\n",
      "step 200 loss 9.0030\n",
      "step 200 loss 9.0030\n",
      "step 300 loss 7.3714\n",
      "step 300 loss 7.3714\n",
      "step 400 loss 7.1246\n",
      "step 400 loss 7.1246\n",
      "step 500 loss 7.0368\n",
      "step 500 loss 7.0368\n",
      "step 600 loss 7.0333\n",
      "step 600 loss 7.0333\n",
      "step 700 loss 7.0521\n",
      "step 700 loss 7.0521\n",
      "step 800 loss 6.6674\n",
      "step 800 loss 6.6674\n",
      "step 900 loss 6.7038\n",
      "step 900 loss 6.7038\n",
      "step 1000 loss 6.9740\n",
      "step 1000 loss 6.9740\n",
      "step 1100 loss 6.9789\n",
      "step 1100 loss 6.9789\n",
      "step 1200 loss 7.0886\n",
      "step 1200 loss 7.0886\n",
      "step 1300 loss 7.0563\n",
      "step 1300 loss 7.0563\n",
      "step 1400 loss 7.1430\n",
      "step 1400 loss 7.1430\n",
      "step 1500 loss 6.4978\n",
      "step 1500 loss 6.4978\n",
      "step 1600 loss 6.8266\n",
      "step 1600 loss 6.8266\n",
      "step 1700 loss 6.8633\n",
      "step 1700 loss 6.8633\n",
      "step 1800 loss 6.5565\n",
      "step 1800 loss 6.5565\n",
      "step 1900 loss 6.4279\n",
      "step 1900 loss 6.4279\n",
      "step 2000 loss 6.3318\n",
      "step 2000 loss 6.3318\n",
      "step 2100 loss 6.2026\n",
      "step 2100 loss 6.2026\n",
      "step 2200 loss 6.0747\n",
      "step 2200 loss 6.0747\n",
      "step 2300 loss 5.9528\n",
      "step 2300 loss 5.9528\n",
      "step 2400 loss 6.2040\n",
      "step 2400 loss 6.2040\n",
      "step 2500 loss 5.8914\n",
      "step 2500 loss 5.8914\n",
      "step 2600 loss 5.9268\n",
      "step 2600 loss 5.9268\n",
      "step 2700 loss 6.0932\n",
      "step 2700 loss 6.0932\n",
      "step 2800 loss 5.7882\n",
      "step 2800 loss 5.7882\n",
      "step 2900 loss 5.6277\n",
      "step 2900 loss 5.6277\n",
      "step 3000 loss 5.7056\n",
      "step 3000 loss 5.7056\n",
      "step 3100 loss 5.7452\n",
      "step 3100 loss 5.7452\n",
      "step 3200 loss 5.7797\n",
      "step 3200 loss 5.7797\n",
      "step 3300 loss 5.8797\n",
      "step 3300 loss 5.8797\n",
      "step 3400 loss 5.6057\n",
      "step 3400 loss 5.6057\n",
      "step 3500 loss 5.5044\n",
      "step 3500 loss 5.5044\n",
      "step 3600 loss 5.5630\n",
      "step 3600 loss 5.5630\n",
      "step 3700 loss 5.4213\n",
      "step 3700 loss 5.4213\n",
      "step 3800 loss 5.7930\n",
      "step 3800 loss 5.7930\n",
      "step 3900 loss 5.5603\n",
      "step 3900 loss 5.5603\n",
      "step 4000 loss 5.8063\n",
      "step 4000 loss 5.8063\n",
      "step 4100 loss 5.4110\n",
      "step 4100 loss 5.4110\n",
      "step 4200 loss 5.4784\n",
      "step 4200 loss 5.4784\n",
      "step 4300 loss 5.3545\n",
      "step 4300 loss 5.3545\n",
      "step 4400 loss 5.4875\n",
      "step 4400 loss 5.4875\n",
      "step 4500 loss 5.1875\n",
      "step 4500 loss 5.1875\n",
      "step 4600 loss 5.6021\n",
      "step 4600 loss 5.6021\n",
      "step 4700 loss 5.8279\n",
      "step 4700 loss 5.8279\n",
      "step 4800 loss 5.3406\n",
      "step 4800 loss 5.3406\n",
      "step 4900 loss 5.1965\n",
      "step 4900 loss 5.1965\n",
      "step 5000 loss 5.3114\n",
      "step 5000 loss 5.3114\n",
      "step 5100 loss 5.7644\n",
      "step 5100 loss 5.7644\n",
      "step 5200 loss 4.5085\n",
      "step 5200 loss 4.5085\n",
      "step 5300 loss 5.3724\n",
      "step 5300 loss 5.3724\n",
      "step 5400 loss 5.3938\n",
      "step 5400 loss 5.3938\n",
      "step 5500 loss 5.0614\n",
      "step 5500 loss 5.0614\n",
      "step 5600 loss 5.2255\n",
      "step 5600 loss 5.2255\n",
      "step 5700 loss 5.3925\n",
      "step 5700 loss 5.3925\n",
      "step 5800 loss 5.4656\n",
      "step 5800 loss 5.4656\n",
      "step 5900 loss 5.2796\n",
      "step 5900 loss 5.2796\n",
      "step 6000 loss 5.7226\n",
      "step 6000 loss 5.7226\n",
      "step 6100 loss 5.4796\n",
      "step 6100 loss 5.4796\n",
      "step 6200 loss 5.0951\n",
      "step 6200 loss 5.0951\n",
      "step 6300 loss 5.4202\n",
      "step 6300 loss 5.4202\n",
      "step 6400 loss 5.1742\n",
      "step 6400 loss 5.1742\n",
      "step 6500 loss 5.4818\n",
      "step 6500 loss 5.4818\n",
      "step 6600 loss 5.4851\n",
      "step 6600 loss 5.4851\n",
      "step 6700 loss 5.4507\n",
      "step 6700 loss 5.4507\n",
      "step 6800 loss 5.2947\n",
      "step 6800 loss 5.2947\n",
      "step 6900 loss 5.3320\n",
      "step 6900 loss 5.3320\n",
      "step 7000 loss 5.2499\n",
      "step 7000 loss 5.2499\n",
      "step 7100 loss 5.1523\n",
      "step 7100 loss 5.1523\n",
      "step 7200 loss 5.2258\n",
      "step 7200 loss 5.2258\n",
      "step 7300 loss 5.1321\n",
      "step 7300 loss 5.1321\n",
      "step 7400 loss 5.2330\n",
      "step 7400 loss 5.2330\n",
      "step 7500 loss 4.8913\n",
      "step 7500 loss 4.8913\n",
      "step 7600 loss 5.2932\n",
      "step 7600 loss 5.2932\n",
      "step 7700 loss 4.9521\n",
      "step 7700 loss 4.9521\n",
      "step 7800 loss 5.0784\n",
      "step 7800 loss 5.0784\n",
      "step 7900 loss 4.9258\n",
      "step 7900 loss 4.9258\n",
      "step 8000 loss 5.0618\n",
      "step 8000 loss 5.0618\n",
      "step 8100 loss 4.3890\n",
      "step 8100 loss 4.3890\n",
      "step 8200 loss 4.9093\n",
      "step 8200 loss 4.9093\n",
      "step 8300 loss 5.2448\n",
      "step 8300 loss 5.2448\n",
      "step 8400 loss 5.1938\n",
      "step 8400 loss 5.1938\n",
      "step 8500 loss 5.0960\n",
      "step 8500 loss 5.0960\n",
      "step 8600 loss 4.9939\n",
      "step 8600 loss 4.9939\n",
      "step 8700 loss 5.2428\n",
      "step 8700 loss 5.2428\n",
      "step 8800 loss 4.8632\n",
      "step 8800 loss 4.8632\n",
      "step 8900 loss 5.0204\n",
      "step 8900 loss 5.0204\n",
      "step 9000 loss 4.8860\n",
      "step 9000 loss 4.8860\n",
      "step 9100 loss 5.0564\n",
      "step 9100 loss 5.0564\n",
      "step 9200 loss 5.2313\n",
      "step 9200 loss 5.2313\n",
      "step 9300 loss 4.7233\n",
      "step 9300 loss 4.7233\n",
      "step 9400 loss 5.0629\n",
      "step 9400 loss 5.0629\n",
      "step 9500 loss 4.5244\n",
      "step 9500 loss 4.5244\n",
      "step 9600 loss 4.8152\n",
      "step 9600 loss 4.8152\n",
      "step 9700 loss 4.7191\n",
      "step 9700 loss 4.7191\n",
      "step 9800 loss 4.8380\n",
      "step 9800 loss 4.8380\n",
      "step 9900 loss 4.7101\n",
      "step 9900 loss 4.7101\n",
      "step 10000 loss 4.9637\n",
      "step 10000 loss 4.9637\n",
      "Saved checkpoint: checkpoints/transformer/epoch_1.pt\n",
      "Epoch 2/5\n",
      "Saved checkpoint: checkpoints/transformer/epoch_1.pt\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa0541a083e45559289c7880283d611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10100 loss 4.5663\n",
      "step 10200 loss 5.2073\n",
      "step 10200 loss 5.2073\n",
      "step 10300 loss 4.6707\n",
      "step 10300 loss 4.6707\n",
      "step 10400 loss 5.2303\n",
      "step 10400 loss 5.2303\n",
      "step 10500 loss 5.1305\n",
      "step 10500 loss 5.1305\n",
      "step 10600 loss 4.9499\n",
      "step 10600 loss 4.9499\n",
      "step 10700 loss 4.6996\n",
      "step 10700 loss 4.6996\n",
      "step 10800 loss 4.4218\n",
      "step 10800 loss 4.4218\n",
      "step 10900 loss 4.6387\n",
      "step 10900 loss 4.6387\n",
      "step 11000 loss 5.0275\n",
      "step 11000 loss 5.0275\n",
      "step 11100 loss 5.0237\n",
      "step 11100 loss 5.0237\n",
      "step 11200 loss 4.9280\n",
      "step 11200 loss 4.9280\n",
      "step 11300 loss 4.9408\n",
      "step 11300 loss 4.9408\n",
      "step 11400 loss 4.7442\n",
      "step 11400 loss 4.7442\n",
      "step 11500 loss 4.2394\n",
      "step 11500 loss 4.2394\n",
      "step 11600 loss 4.5314\n",
      "step 11600 loss 4.5314\n",
      "step 11700 loss 4.7785\n",
      "step 11700 loss 4.7785\n",
      "step 11800 loss 4.6663\n",
      "step 11800 loss 4.6663\n",
      "step 11900 loss 4.8101\n",
      "step 11900 loss 4.8101\n",
      "step 12000 loss 4.9758\n",
      "step 12000 loss 4.9758\n",
      "step 12100 loss 4.8926\n",
      "step 12100 loss 4.8926\n",
      "step 12200 loss 4.8437\n",
      "step 12200 loss 4.8437\n",
      "step 12300 loss 4.8470\n",
      "step 12300 loss 4.8470\n",
      "step 12400 loss 5.0291\n",
      "step 12400 loss 5.0291\n",
      "step 12500 loss 4.8156\n",
      "step 12500 loss 4.8156\n",
      "step 12600 loss 4.9214\n",
      "step 12600 loss 4.9214\n",
      "step 12700 loss 5.1234\n",
      "step 12700 loss 5.1234\n",
      "step 12800 loss 4.8715\n",
      "step 12800 loss 4.8715\n",
      "step 12900 loss 4.7651\n",
      "step 12900 loss 4.7651\n",
      "step 13000 loss 4.7194\n",
      "step 13000 loss 4.7194\n",
      "step 13100 loss 4.8021\n",
      "step 13100 loss 4.8021\n",
      "step 13200 loss 4.8700\n",
      "step 13200 loss 4.8700\n",
      "step 13300 loss 4.9489\n",
      "step 13300 loss 4.9489\n",
      "step 13400 loss 4.8146\n",
      "step 13400 loss 4.8146\n",
      "step 13500 loss 4.7102\n",
      "step 13500 loss 4.7102\n",
      "step 13600 loss 4.7346\n",
      "step 13600 loss 4.7346\n",
      "step 13700 loss 4.4873\n",
      "step 13700 loss 4.4873\n",
      "step 13800 loss 5.0124\n",
      "step 13800 loss 5.0124\n",
      "step 13900 loss 4.7408\n",
      "step 13900 loss 4.7408\n",
      "step 14000 loss 5.1291\n",
      "step 14000 loss 5.1291\n",
      "step 14100 loss 4.6959\n",
      "step 14100 loss 4.6959\n",
      "step 14200 loss 4.6794\n",
      "step 14200 loss 4.6794\n",
      "step 14300 loss 4.6387\n",
      "step 14300 loss 4.6387\n",
      "step 14400 loss 4.7054\n",
      "step 14400 loss 4.7054\n",
      "step 14500 loss 4.3036\n",
      "step 14500 loss 4.3036\n",
      "step 14600 loss 4.8725\n",
      "step 14600 loss 4.8725\n",
      "step 14700 loss 5.0756\n",
      "step 14700 loss 5.0756\n",
      "step 14800 loss 4.5242\n",
      "step 14800 loss 4.5242\n",
      "step 14900 loss 4.4190\n",
      "step 14900 loss 4.4190\n",
      "step 15000 loss 4.5491\n",
      "step 15000 loss 4.5491\n",
      "step 15100 loss 5.0328\n",
      "step 15100 loss 5.0328\n",
      "step 15200 loss 3.9084\n",
      "step 15200 loss 3.9084\n",
      "step 15300 loss 4.6740\n",
      "step 15300 loss 4.6740\n",
      "step 15400 loss 4.7348\n",
      "step 15400 loss 4.7348\n",
      "step 15500 loss 4.3815\n",
      "step 15500 loss 4.3815\n",
      "step 15600 loss 4.4640\n",
      "step 15600 loss 4.4640\n",
      "step 15700 loss 4.6639\n",
      "step 15700 loss 4.6639\n",
      "step 15800 loss 4.7958\n",
      "step 15800 loss 4.7958\n",
      "step 15900 loss 4.5607\n",
      "step 15900 loss 4.5607\n",
      "step 16000 loss 5.0903\n",
      "step 16000 loss 5.0903\n",
      "step 16100 loss 4.8232\n",
      "step 16100 loss 4.8232\n",
      "step 16200 loss 4.3941\n",
      "step 16200 loss 4.3941\n",
      "step 16300 loss 4.7934\n",
      "step 16300 loss 4.7934\n",
      "step 16400 loss 4.4564\n",
      "step 16400 loss 4.4564\n",
      "step 16500 loss 4.7610\n",
      "step 16500 loss 4.7610\n",
      "step 16600 loss 4.7831\n",
      "step 16600 loss 4.7831\n",
      "step 16700 loss 4.7985\n",
      "step 16700 loss 4.7985\n",
      "step 16800 loss 4.5734\n",
      "step 16800 loss 4.5734\n",
      "step 16900 loss 4.6738\n",
      "step 16900 loss 4.6738\n",
      "step 17000 loss 4.6435\n",
      "step 17000 loss 4.6435\n",
      "step 17100 loss 4.5034\n",
      "step 17100 loss 4.5034\n",
      "step 17200 loss 4.5093\n",
      "step 17200 loss 4.5093\n",
      "step 17300 loss 4.4581\n",
      "step 17300 loss 4.4581\n",
      "step 17400 loss 4.6594\n",
      "step 17400 loss 4.6594\n",
      "step 17500 loss 4.3517\n",
      "step 17500 loss 4.3517\n",
      "step 17600 loss 4.6734\n",
      "step 17600 loss 4.6734\n",
      "step 17700 loss 4.3210\n",
      "step 17700 loss 4.3210\n",
      "step 17800 loss 4.4770\n",
      "step 17800 loss 4.4770\n",
      "step 17900 loss 4.3139\n",
      "step 17900 loss 4.3139\n",
      "step 18000 loss 4.4928\n",
      "step 18000 loss 4.4928\n",
      "step 18100 loss 3.8203\n",
      "step 18100 loss 3.8203\n",
      "step 18200 loss 4.3113\n",
      "step 18200 loss 4.3113\n",
      "step 18300 loss 4.6383\n",
      "step 18300 loss 4.6383\n",
      "step 18400 loss 4.6794\n",
      "step 18400 loss 4.6794\n",
      "step 18500 loss 4.5497\n",
      "step 18500 loss 4.5497\n",
      "step 18600 loss 4.4713\n",
      "step 18600 loss 4.4713\n",
      "step 18700 loss 4.6865\n",
      "step 18700 loss 4.6865\n",
      "step 18800 loss 4.3154\n",
      "step 18800 loss 4.3154\n",
      "step 18900 loss 4.4519\n",
      "step 18900 loss 4.4519\n",
      "step 19000 loss 4.3367\n",
      "step 19000 loss 4.3367\n",
      "step 19100 loss 4.5603\n",
      "step 19100 loss 4.5603\n",
      "step 19200 loss 4.7072\n",
      "step 19200 loss 4.7072\n",
      "step 19300 loss 4.1889\n",
      "step 19300 loss 4.1889\n",
      "step 19400 loss 4.5808\n",
      "step 19400 loss 4.5808\n",
      "step 19500 loss 4.0325\n",
      "step 19500 loss 4.0325\n",
      "step 19600 loss 4.2667\n",
      "step 19600 loss 4.2667\n",
      "step 19700 loss 4.1859\n",
      "step 19700 loss 4.1859\n",
      "step 19800 loss 4.3704\n",
      "step 19800 loss 4.3704\n",
      "step 19900 loss 4.2307\n",
      "step 19900 loss 4.2307\n",
      "step 20000 loss 4.5440\n",
      "step 20000 loss 4.5440\n",
      "Saved checkpoint: checkpoints/transformer/epoch_2.pt\n",
      "Epoch 3/5\n",
      "Saved checkpoint: checkpoints/transformer/epoch_2.pt\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1b651e13824c368c9dce4b0d52c75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c9a6aa9e-67f9-4f5b-af2f-077893df2b9a)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c9a6aa9e-67f9-4f5b-af2f-077893df2b9a)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: dad360ae-fd06-4a4e-82b4-9352eeb41846)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: dad360ae-fd06-4a4e-82b4-9352eeb41846)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: fc8f745e-7403-4f28-ae52-1388c21d779c)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: fc8f745e-7403-4f28-ae52-1388c21d779c)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20100 loss 4.0195\n",
      "step 20200 loss 4.6922\n",
      "step 20200 loss 4.6922\n",
      "step 20300 loss 4.2175\n",
      "step 20300 loss 4.2175\n",
      "step 20400 loss 4.7771\n",
      "step 20400 loss 4.7771\n",
      "step 20500 loss 4.6266\n",
      "step 20500 loss 4.6266\n",
      "step 20600 loss 4.4314\n",
      "step 20600 loss 4.4314\n",
      "step 20700 loss 4.2002\n",
      "step 20700 loss 4.2002\n",
      "step 20800 loss 3.9589\n",
      "step 20800 loss 3.9589\n",
      "step 20900 loss 4.1798\n",
      "step 20900 loss 4.1798\n",
      "step 21000 loss 4.5862\n",
      "step 21000 loss 4.5862\n",
      "step 21100 loss 4.5589\n",
      "step 21100 loss 4.5589\n",
      "step 21200 loss 4.4931\n",
      "step 21200 loss 4.4931\n",
      "step 21300 loss 4.4629\n",
      "step 21300 loss 4.4629\n",
      "step 21400 loss 4.3088\n",
      "step 21400 loss 4.3088\n",
      "step 21500 loss 3.8033\n",
      "step 21500 loss 3.8033\n",
      "step 21600 loss 4.0765\n",
      "step 21600 loss 4.0765\n",
      "step 21700 loss 4.3521\n",
      "step 21700 loss 4.3521\n",
      "step 21800 loss 4.2316\n",
      "step 21800 loss 4.2316\n",
      "step 21900 loss 4.3836\n",
      "step 21900 loss 4.3836\n",
      "step 22000 loss 4.5509\n",
      "step 22000 loss 4.5509\n",
      "step 22100 loss 4.4803\n",
      "step 22100 loss 4.4803\n",
      "step 22200 loss 4.4487\n",
      "step 22200 loss 4.4487\n",
      "step 22300 loss 4.4609\n",
      "step 22300 loss 4.4609\n",
      "step 22400 loss 4.5269\n",
      "step 22400 loss 4.5269\n",
      "step 22500 loss 4.3450\n",
      "step 22500 loss 4.3450\n",
      "step 22600 loss 4.5518\n",
      "step 22600 loss 4.5518\n",
      "step 22700 loss 4.7350\n",
      "step 22700 loss 4.7350\n",
      "step 22800 loss 4.4137\n",
      "step 22800 loss 4.4137\n",
      "step 22900 loss 4.3590\n",
      "step 22900 loss 4.3590\n",
      "step 23000 loss 4.2415\n",
      "step 23000 loss 4.2415\n",
      "step 23100 loss 4.4112\n",
      "step 23100 loss 4.4112\n",
      "step 23200 loss 4.5052\n",
      "step 23200 loss 4.5052\n",
      "step 23300 loss 4.5258\n",
      "step 23300 loss 4.5258\n",
      "step 23400 loss 4.3910\n",
      "step 23400 loss 4.3910\n",
      "step 23500 loss 4.3528\n",
      "step 23500 loss 4.3528\n",
      "step 23600 loss 4.3451\n",
      "step 23600 loss 4.3451\n",
      "step 23700 loss 4.0273\n",
      "step 23700 loss 4.0273\n",
      "step 23800 loss 4.6801\n",
      "step 23800 loss 4.6801\n",
      "step 23900 loss 4.3799\n",
      "step 23900 loss 4.3799\n",
      "step 24000 loss 4.8070\n",
      "step 24000 loss 4.8070\n",
      "step 24100 loss 4.3660\n",
      "step 24100 loss 4.3660\n",
      "step 24200 loss 4.3491\n",
      "step 24200 loss 4.3491\n",
      "step 24300 loss 4.2703\n",
      "step 24300 loss 4.2703\n",
      "step 24400 loss 4.2362\n",
      "step 24400 loss 4.2362\n",
      "step 24500 loss 3.9011\n",
      "step 24500 loss 3.9011\n",
      "step 24600 loss 4.5114\n",
      "step 24600 loss 4.5114\n",
      "step 24700 loss 4.6729\n",
      "step 24700 loss 4.6729\n",
      "step 24800 loss 4.1533\n",
      "step 24800 loss 4.1533\n",
      "step 24900 loss 4.0022\n",
      "step 24900 loss 4.0022\n",
      "step 25000 loss 4.1645\n",
      "step 25000 loss 4.1645\n",
      "step 25100 loss 4.6458\n",
      "step 25100 loss 4.6458\n",
      "step 25200 loss 3.6389\n",
      "step 25200 loss 3.6389\n",
      "step 25300 loss 4.3384\n",
      "step 25300 loss 4.3384\n",
      "step 25400 loss 4.4042\n",
      "step 25400 loss 4.4042\n",
      "step 25500 loss 3.9323\n",
      "step 25500 loss 3.9323\n",
      "step 25600 loss 4.0751\n",
      "step 25600 loss 4.0751\n",
      "step 25700 loss 4.2984\n",
      "step 25700 loss 4.2984\n",
      "step 25800 loss 4.3956\n",
      "step 25800 loss 4.3956\n",
      "step 25900 loss 4.1881\n",
      "step 25900 loss 4.1881\n",
      "step 26000 loss 4.7332\n",
      "step 26000 loss 4.7332\n",
      "step 26100 loss 4.4668\n",
      "step 26100 loss 4.4668\n",
      "step 26200 loss 4.0573\n",
      "step 26200 loss 4.0573\n",
      "step 26300 loss 4.4441\n",
      "step 26300 loss 4.4441\n",
      "step 26400 loss 4.0767\n",
      "step 26400 loss 4.0767\n",
      "step 26500 loss 4.4028\n",
      "step 26500 loss 4.4028\n",
      "step 26600 loss 4.4124\n",
      "step 26600 loss 4.4124\n",
      "step 26700 loss 4.4650\n",
      "step 26700 loss 4.4650\n",
      "step 26800 loss 4.1578\n",
      "step 26800 loss 4.1578\n",
      "step 26900 loss 4.3239\n",
      "step 26900 loss 4.3239\n",
      "step 27000 loss 4.2809\n",
      "step 27000 loss 4.2809\n",
      "step 27100 loss 4.1157\n",
      "step 27100 loss 4.1157\n",
      "step 27200 loss 4.1941\n",
      "step 27200 loss 4.1941\n",
      "step 27300 loss 4.1074\n",
      "step 27300 loss 4.1074\n",
      "step 27400 loss 4.3397\n",
      "step 27400 loss 4.3397\n",
      "step 27500 loss 3.9891\n",
      "step 27500 loss 3.9891\n",
      "step 27600 loss 4.2756\n",
      "step 27600 loss 4.2756\n",
      "step 27700 loss 3.9739\n",
      "step 27700 loss 3.9739\n",
      "step 27800 loss 4.1242\n",
      "step 27800 loss 4.1242\n",
      "step 27900 loss 3.9408\n",
      "step 27900 loss 3.9408\n",
      "step 28000 loss 4.1390\n",
      "step 28000 loss 4.1390\n",
      "step 28100 loss 3.5218\n",
      "step 28100 loss 3.5218\n",
      "step 28200 loss 3.9567\n",
      "step 28200 loss 3.9567\n",
      "step 28300 loss 4.3103\n",
      "step 28300 loss 4.3103\n",
      "step 28400 loss 4.2840\n",
      "step 28400 loss 4.2840\n",
      "step 28500 loss 4.2199\n",
      "step 28500 loss 4.2199\n",
      "step 28600 loss 4.2124\n",
      "step 28600 loss 4.2124\n",
      "step 28700 loss 4.3830\n",
      "step 28700 loss 4.3830\n",
      "step 28800 loss 4.0128\n",
      "step 28800 loss 4.0128\n",
      "step 28900 loss 4.1181\n",
      "step 28900 loss 4.1181\n",
      "step 29000 loss 3.9537\n",
      "step 29000 loss 3.9537\n",
      "step 29100 loss 4.2571\n",
      "step 29100 loss 4.2571\n",
      "step 29200 loss 4.4201\n",
      "step 29200 loss 4.4201\n",
      "step 29300 loss 3.8594\n",
      "step 29300 loss 3.8594\n",
      "step 29400 loss 4.2756\n",
      "step 29400 loss 4.2756\n",
      "step 29500 loss 3.7222\n",
      "step 29500 loss 3.7222\n",
      "step 29600 loss 3.9391\n",
      "step 29600 loss 3.9391\n",
      "step 29700 loss 3.8572\n",
      "step 29700 loss 3.8572\n",
      "step 29800 loss 4.0918\n",
      "step 29800 loss 4.0918\n",
      "step 29900 loss 3.9456\n",
      "step 29900 loss 3.9456\n",
      "step 30000 loss 4.2546\n",
      "step 30000 loss 4.2546\n",
      "Saved checkpoint: checkpoints/transformer/epoch_3.pt\n",
      "Epoch 4/5\n",
      "Saved checkpoint: checkpoints/transformer/epoch_3.pt\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05870bd29f842dcb71fae051eeca7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2d781aea-3d25-4cc9-9abd-9adec03051e4)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2d781aea-3d25-4cc9-9abd-9adec03051e4)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7a1dd98a-b6fd-4189-9766-b5e74efe7c1f)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7a1dd98a-b6fd-4189-9766-b5e74efe7c1f)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3bed7fee-7e07-4fdf-95b9-9c7d11902891)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 3bed7fee-7e07-4fdf-95b9-9c7d11902891)')' thrown while requesting GET https://huggingface.co/datasets/mikex86/stackoverflow-posts/resolve/9e791fe8997879cf127e2a0b006bad3484bbda32/stackoverflow-posts-00000-of-00058.parquet\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 30100 loss 3.6896\n",
      "step 30200 loss 4.3520\n",
      "step 30200 loss 4.3520\n",
      "step 30300 loss 3.9746\n",
      "step 30300 loss 3.9746\n",
      "step 30400 loss 4.4095\n",
      "step 30400 loss 4.4095\n",
      "step 30500 loss 4.2988\n",
      "step 30500 loss 4.2988\n",
      "step 30600 loss 4.0426\n",
      "step 30600 loss 4.0426\n",
      "step 30700 loss 3.8768\n",
      "step 30700 loss 3.8768\n",
      "step 30800 loss 3.6687\n",
      "step 30800 loss 3.6687\n",
      "step 30900 loss 3.9113\n",
      "step 30900 loss 3.9113\n",
      "step 31000 loss 4.3490\n",
      "step 31000 loss 4.3490\n",
      "step 31100 loss 4.3274\n",
      "step 31100 loss 4.3274\n",
      "step 31200 loss 4.2079\n",
      "step 31200 loss 4.2079\n",
      "step 31300 loss 4.1481\n",
      "step 31300 loss 4.1481\n",
      "step 31400 loss 4.0659\n",
      "step 31400 loss 4.0659\n",
      "step 31500 loss 3.5830\n",
      "step 31500 loss 3.5830\n",
      "step 31600 loss 3.7826\n",
      "step 31600 loss 3.7826\n",
      "step 31700 loss 4.0555\n",
      "step 31700 loss 4.0555\n",
      "step 31800 loss 3.9577\n",
      "step 31800 loss 3.9577\n",
      "step 31900 loss 4.1347\n",
      "step 31900 loss 4.1347\n",
      "step 32000 loss 4.3105\n",
      "step 32000 loss 4.3105\n",
      "step 32100 loss 4.1993\n",
      "step 32100 loss 4.1993\n",
      "step 32200 loss 4.1876\n",
      "step 32200 loss 4.1876\n",
      "step 32300 loss 4.2119\n",
      "step 32300 loss 4.2119\n",
      "step 32400 loss 4.2847\n",
      "step 32400 loss 4.2847\n",
      "step 32500 loss 4.0497\n",
      "step 32500 loss 4.0497\n",
      "step 32600 loss 4.2422\n",
      "step 32600 loss 4.2422\n",
      "step 32700 loss 4.4480\n",
      "step 32700 loss 4.4480\n",
      "step 32800 loss 4.1155\n",
      "step 32800 loss 4.1155\n",
      "step 32900 loss 4.0993\n",
      "step 32900 loss 4.0993\n",
      "step 33000 loss 3.9710\n",
      "step 33000 loss 3.9710\n",
      "step 33100 loss 4.1715\n",
      "step 33100 loss 4.1715\n",
      "step 33200 loss 4.3139\n",
      "step 33200 loss 4.3139\n",
      "step 33300 loss 4.2307\n",
      "step 33300 loss 4.2307\n",
      "step 33400 loss 4.1004\n",
      "step 33400 loss 4.1004\n",
      "step 33500 loss 4.0563\n",
      "step 33500 loss 4.0563\n",
      "step 33600 loss 4.0217\n",
      "step 33600 loss 4.0217\n",
      "step 33700 loss 3.6473\n",
      "step 33700 loss 3.6473\n",
      "step 33800 loss 4.4799\n",
      "step 33800 loss 4.4799\n",
      "step 33900 loss 4.1231\n",
      "step 33900 loss 4.1231\n",
      "step 34000 loss 4.5611\n",
      "step 34000 loss 4.5611\n",
      "step 34100 loss 4.1255\n",
      "step 34100 loss 4.1255\n",
      "step 34200 loss 4.1044\n",
      "step 34200 loss 4.1044\n",
      "step 34300 loss 4.0405\n",
      "step 34300 loss 4.0405\n",
      "step 34400 loss 3.8994\n",
      "step 34400 loss 3.8994\n",
      "step 34500 loss 3.6254\n",
      "step 34500 loss 3.6254\n",
      "step 34600 loss 4.2340\n",
      "step 34600 loss 4.2340\n",
      "step 34700 loss 4.3795\n",
      "step 34700 loss 4.3795\n",
      "step 34800 loss 3.9247\n",
      "step 34800 loss 3.9247\n",
      "step 34900 loss 3.6928\n",
      "step 34900 loss 3.6928\n",
      "step 35000 loss 3.9348\n",
      "step 35000 loss 3.9348\n",
      "step 35100 loss 4.3994\n",
      "step 35100 loss 4.3994\n",
      "step 35200 loss 3.4206\n",
      "step 35200 loss 3.4206\n",
      "step 35300 loss 4.0912\n",
      "step 35300 loss 4.0912\n",
      "step 35400 loss 4.1162\n",
      "step 35400 loss 4.1162\n",
      "step 35500 loss 3.6599\n",
      "step 35500 loss 3.6599\n",
      "step 35600 loss 3.8642\n",
      "step 35600 loss 3.8642\n",
      "step 35700 loss 4.0685\n",
      "step 35700 loss 4.0685\n",
      "step 35800 loss 4.2175\n",
      "step 35800 loss 4.2175\n",
      "step 35900 loss 4.0004\n",
      "step 35900 loss 4.0004\n",
      "step 36000 loss 4.5011\n",
      "step 36000 loss 4.5011\n",
      "step 36100 loss 4.2186\n",
      "step 36100 loss 4.2186\n",
      "step 36200 loss 3.8147\n",
      "step 36200 loss 3.8147\n",
      "step 36300 loss 4.2275\n",
      "step 36300 loss 4.2275\n",
      "step 36400 loss 3.8556\n",
      "step 36400 loss 3.8556\n",
      "step 36500 loss 4.1968\n",
      "step 36500 loss 4.1968\n",
      "step 36600 loss 4.1354\n",
      "step 36600 loss 4.1354\n",
      "step 36700 loss 4.2393\n",
      "step 36700 loss 4.2393\n",
      "step 36800 loss 3.9028\n",
      "step 36800 loss 3.9028\n",
      "step 36900 loss 4.0459\n",
      "step 36900 loss 4.0459\n",
      "step 37000 loss 3.9810\n",
      "step 37000 loss 3.9810\n",
      "step 37100 loss 3.9086\n",
      "step 37100 loss 3.9086\n",
      "step 37200 loss 3.9678\n",
      "step 37200 loss 3.9678\n",
      "step 37300 loss 3.8629\n",
      "step 37300 loss 3.8629\n",
      "step 37400 loss 4.1105\n",
      "step 37400 loss 4.1105\n",
      "step 37500 loss 3.7687\n",
      "step 37500 loss 3.7687\n",
      "step 37600 loss 4.0151\n",
      "step 37600 loss 4.0151\n",
      "step 37700 loss 3.7193\n",
      "step 37700 loss 3.7193\n",
      "step 37800 loss 3.9348\n",
      "step 37800 loss 3.9348\n",
      "step 37900 loss 3.7103\n",
      "step 37900 loss 3.7103\n",
      "step 38000 loss 3.9005\n",
      "step 38000 loss 3.9005\n",
      "step 38100 loss 3.3378\n",
      "step 38100 loss 3.3378\n",
      "step 38200 loss 3.7050\n",
      "step 38200 loss 3.7050\n",
      "step 38300 loss 4.0947\n",
      "step 38300 loss 4.0947\n",
      "step 38400 loss 3.9841\n",
      "step 38400 loss 3.9841\n",
      "step 38500 loss 4.0170\n",
      "step 38500 loss 4.0170\n",
      "step 38600 loss 3.9794\n",
      "step 38600 loss 3.9794\n",
      "step 38700 loss 4.1959\n",
      "step 38700 loss 4.1959\n",
      "step 38800 loss 3.7709\n",
      "step 38800 loss 3.7709\n",
      "step 38900 loss 3.9193\n",
      "step 38900 loss 3.9193\n",
      "step 39000 loss 3.7271\n",
      "step 39000 loss 3.7271\n",
      "step 39100 loss 4.0313\n",
      "step 39100 loss 4.0313\n",
      "step 39200 loss 4.2191\n",
      "step 39200 loss 4.2191\n",
      "step 39300 loss 3.6721\n",
      "step 39300 loss 3.6721\n",
      "step 39400 loss 4.0532\n",
      "step 39400 loss 4.0532\n",
      "step 39500 loss 3.4791\n",
      "step 39500 loss 3.4791\n",
      "step 39600 loss 3.7094\n",
      "step 39600 loss 3.7094\n",
      "step 39700 loss 3.6246\n",
      "step 39700 loss 3.6246\n",
      "step 39800 loss 3.8441\n",
      "step 39800 loss 3.8441\n",
      "step 39900 loss 3.7920\n",
      "step 39900 loss 3.7920\n",
      "step 40000 loss 4.0793\n",
      "step 40000 loss 4.0793\n",
      "Saved checkpoint: checkpoints/transformer/epoch_4.pt\n",
      "Epoch 5/5\n",
      "Saved checkpoint: checkpoints/transformer/epoch_4.pt\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1f865a0f59429a9f6479515760513f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 40100 loss 3.5020\n",
      "step 40200 loss 4.1087\n",
      "step 40200 loss 4.1087\n",
      "step 40300 loss 3.8078\n",
      "step 40300 loss 3.8078\n",
      "step 40400 loss 4.1644\n",
      "step 40400 loss 4.1644\n",
      "step 40500 loss 3.9975\n",
      "step 40500 loss 3.9975\n",
      "step 40600 loss 3.8347\n",
      "step 40600 loss 3.8347\n",
      "step 40700 loss 3.6256\n",
      "step 40700 loss 3.6256\n",
      "step 40800 loss 3.5484\n",
      "step 40800 loss 3.5484\n",
      "step 40900 loss 3.7593\n",
      "step 40900 loss 3.7593\n",
      "step 41000 loss 4.1752\n",
      "step 41000 loss 4.1752\n",
      "step 41100 loss 4.1405\n",
      "step 41100 loss 4.1405\n",
      "step 41200 loss 4.0305\n",
      "step 41200 loss 4.0305\n",
      "step 41300 loss 3.9065\n",
      "step 41300 loss 3.9065\n",
      "step 41400 loss 3.8749\n",
      "step 41400 loss 3.8749\n",
      "step 41500 loss 3.3885\n",
      "step 41500 loss 3.3885\n",
      "step 41600 loss 3.5791\n",
      "step 41600 loss 3.5791\n",
      "step 41700 loss 3.8986\n",
      "step 41700 loss 3.8986\n",
      "step 41800 loss 3.7314\n",
      "step 41800 loss 3.7314\n",
      "step 41900 loss 3.9423\n",
      "step 41900 loss 3.9423\n",
      "step 42000 loss 4.1004\n",
      "step 42000 loss 4.1004\n",
      "step 42100 loss 4.0363\n",
      "step 42100 loss 4.0363\n",
      "step 42200 loss 4.0230\n",
      "step 42200 loss 4.0230\n",
      "step 42300 loss 4.0328\n",
      "step 42300 loss 4.0328\n",
      "step 42400 loss 4.0523\n",
      "step 42400 loss 4.0523\n",
      "step 42500 loss 3.7853\n",
      "step 42500 loss 3.7853\n",
      "step 42600 loss 4.0630\n",
      "step 42600 loss 4.0630\n",
      "step 42700 loss 4.2949\n",
      "step 42700 loss 4.2949\n",
      "step 42800 loss 3.9739\n",
      "step 42800 loss 3.9739\n",
      "step 42900 loss 3.9120\n",
      "step 42900 loss 3.9120\n",
      "step 43000 loss 3.8089\n",
      "step 43000 loss 3.8089\n",
      "step 43100 loss 3.9668\n",
      "step 43100 loss 3.9668\n",
      "step 43200 loss 4.1269\n",
      "step 43200 loss 4.1269\n",
      "step 43300 loss 4.0349\n",
      "step 43300 loss 4.0349\n",
      "step 43400 loss 3.9207\n",
      "step 43400 loss 3.9207\n",
      "step 43500 loss 3.8986\n",
      "step 43500 loss 3.8986\n",
      "step 43600 loss 3.8302\n",
      "step 43600 loss 3.8302\n",
      "step 43700 loss 3.4574\n",
      "step 43700 loss 3.4574\n",
      "step 43800 loss 4.3267\n",
      "step 43800 loss 4.3267\n",
      "step 43900 loss 3.9604\n",
      "step 43900 loss 3.9604\n",
      "step 44000 loss 4.3977\n",
      "step 44000 loss 4.3977\n",
      "step 44100 loss 3.9962\n",
      "step 44100 loss 3.9962\n",
      "step 44200 loss 3.9324\n",
      "step 44200 loss 3.9324\n",
      "step 44300 loss 3.8608\n",
      "step 44300 loss 3.8608\n",
      "step 44400 loss 3.7095\n",
      "step 44400 loss 3.7095\n",
      "step 44500 loss 3.4220\n",
      "step 44500 loss 3.4220\n",
      "step 44600 loss 4.0441\n",
      "step 44600 loss 4.0441\n",
      "step 44700 loss 4.2039\n",
      "step 44700 loss 4.2039\n",
      "step 44800 loss 3.7210\n",
      "step 44800 loss 3.7210\n",
      "step 44900 loss 3.5326\n",
      "step 44900 loss 3.5326\n",
      "step 45000 loss 3.7865\n",
      "step 45000 loss 3.7865\n",
      "step 45100 loss 4.2026\n",
      "step 45100 loss 4.2026\n",
      "step 45200 loss 3.2778\n",
      "step 45200 loss 3.2778\n",
      "step 45300 loss 3.9188\n",
      "step 45300 loss 3.9188\n",
      "step 45400 loss 3.9499\n",
      "step 45400 loss 3.9499\n",
      "step 45500 loss 3.4887\n",
      "step 45500 loss 3.4887\n",
      "step 45600 loss 3.6509\n",
      "step 45600 loss 3.6509\n",
      "step 45700 loss 3.9584\n",
      "step 45700 loss 3.9584\n",
      "step 45800 loss 4.0246\n",
      "step 45800 loss 4.0246\n",
      "step 45900 loss 3.8149\n",
      "step 45900 loss 3.8149\n",
      "step 46000 loss 4.3486\n",
      "step 46000 loss 4.3486\n",
      "step 46100 loss 4.0445\n",
      "step 46100 loss 4.0445\n",
      "step 46200 loss 3.6474\n",
      "step 46200 loss 3.6474\n",
      "step 46300 loss 4.0630\n",
      "step 46300 loss 4.0630\n",
      "step 46400 loss 3.6633\n",
      "step 46400 loss 3.6633\n",
      "step 46500 loss 4.0543\n",
      "step 46500 loss 4.0543\n",
      "step 46600 loss 3.9850\n",
      "step 46600 loss 3.9850\n",
      "step 46700 loss 4.0343\n",
      "step 46700 loss 4.0343\n",
      "step 46800 loss 3.7101\n",
      "step 46800 loss 3.7101\n",
      "step 46900 loss 3.9121\n",
      "step 46900 loss 3.9121\n",
      "step 47000 loss 3.8183\n",
      "step 47000 loss 3.8183\n",
      "step 47100 loss 3.7374\n",
      "step 47100 loss 3.7374\n",
      "step 47200 loss 3.7632\n",
      "step 47200 loss 3.7632\n",
      "step 47300 loss 3.7147\n",
      "step 47300 loss 3.7147\n",
      "step 47400 loss 3.9568\n",
      "step 47400 loss 3.9568\n",
      "step 47500 loss 3.6065\n",
      "step 47500 loss 3.6065\n",
      "step 47600 loss 3.8322\n",
      "step 47600 loss 3.8322\n",
      "step 47700 loss 3.5583\n",
      "step 47700 loss 3.5583\n",
      "step 47800 loss 3.8040\n",
      "step 47800 loss 3.8040\n",
      "step 47900 loss 3.5572\n",
      "step 47900 loss 3.5572\n",
      "step 48000 loss 3.7576\n",
      "step 48000 loss 3.7576\n",
      "step 48100 loss 3.2482\n",
      "step 48100 loss 3.2482\n",
      "step 48200 loss 3.5834\n",
      "step 48200 loss 3.5834\n",
      "step 48300 loss 3.9349\n",
      "step 48300 loss 3.9349\n",
      "step 48400 loss 3.7739\n",
      "step 48400 loss 3.7739\n",
      "step 48500 loss 3.8534\n",
      "step 48500 loss 3.8534\n",
      "step 48600 loss 3.8739\n",
      "step 48600 loss 3.8739\n",
      "step 48700 loss 4.0458\n",
      "step 48700 loss 4.0458\n",
      "step 48800 loss 3.5964\n",
      "step 48800 loss 3.5964\n",
      "step 48900 loss 3.7740\n",
      "step 48900 loss 3.7740\n",
      "step 49000 loss 3.6089\n",
      "step 49000 loss 3.6089\n",
      "step 49100 loss 3.8790\n",
      "step 49100 loss 3.8790\n",
      "step 49200 loss 4.1013\n",
      "step 49200 loss 4.1013\n",
      "step 49300 loss 3.5606\n",
      "step 49300 loss 3.5606\n",
      "step 49400 loss 3.8439\n",
      "step 49400 loss 3.8439\n",
      "step 49500 loss 3.3855\n",
      "step 49500 loss 3.3855\n",
      "step 49600 loss 3.5711\n",
      "step 49600 loss 3.5711\n",
      "step 49700 loss 3.4757\n",
      "step 49700 loss 3.4757\n",
      "step 49800 loss 3.7211\n",
      "step 49800 loss 3.7211\n",
      "step 49900 loss 3.6657\n",
      "step 49900 loss 3.6657\n",
      "step 50000 loss 3.9023\n",
      "step 50000 loss 3.9023\n",
      "Saved checkpoint: checkpoints/transformer/epoch_5.pt\n",
      "Training complete. Saved to: outputs/transformer/final.pt\n",
      "Saved checkpoint: checkpoints/transformer/epoch_5.pt\n",
      "Training complete. Saved to: outputs/transformer/final.pt\n"
     ]
    }
   ],
   "source": [
    " # Full training (Transformer) using shared utilities\n",
    "trans_final = train_streamed_lm(\n",
    "    model=trans,\n",
    "    tokenizer=tokenizer,\n",
    "    config=tr_cfg.__dict__,\n",
    "    ckpt_dir=\"checkpoints/transformer\",\n",
    "    final_dir=\"outputs/transformer\",\n",
    "    batch_size=16,\n",
    "    max_length=256,\n",
    "    steps_per_epoch=10000,\n",
    "    num_epochs=5,\n",
    "    save_every=1,\n",
    "    lr=3e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3513e17",
   "metadata": {},
   "source": [
    "5. Inference and Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfed1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM ->\n",
      " code from using a time, you just to this can a line by 3, you [, I wrote a web. I have a previous page, an by text is by: is the time in a 2 line and have a one method (2, a.c.0.0, 2, for...,, on, -, and, it, a text is not to the old\n",
      "\n",
      "Transformer ->\n",
      "   It is very powerful for this purpose.  The reason that I'm aware of are in the past of what you're going to be able to do. \n",
      "\n",
      "I have seen a tool that is the best way for creating some Python functions for writing a library to emulate that.  Even if it's a lot of C and C++ code then I can understand it, because of the\n",
      "\n",
      "Transformer ->\n",
      "   It is very powerful for this purpose.  The reason that I'm aware of are in the past of what you're going to be able to do. \n",
      "\n",
      "I have seen a tool that is the best way for creating some Python functions for writing a library to emulate that.  Even if it's a lot of C and C++ code then I can understand it, because of the\n"
     ]
    }
   ],
   "source": [
    "# Inference: quick generation using shared utility\n",
    "prompt = \"Explain what a Python generator is and provide a short example.\"\n",
    "print(\"LSTM ->\\n\", generate_text(lstm, tokenizer, prompt, max_new_tokens=80))\n",
    "print(\"\\nTransformer ->\\n\", generate_text(trans, tokenizer, prompt, max_new_tokens=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342c061",
   "metadata": {},
   "source": [
    "6. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4408e37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lstm_tok_per_s': 42405.42756745674, 'transformer_tok_per_s': 69119.05862183005}\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison using shared utility\n",
    "lstm_tok_s = measure_throughput(lstm, tokenizer.vocab_size)\n",
    "trans_tok_s = measure_throughput(trans, tokenizer.vocab_size)\n",
    "print({\"lstm_tok_per_s\": lstm_tok_s, \"transformer_tok_per_s\": trans_tok_s})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a49775",
   "metadata": {},
   "source": [
    "7. Test-set evaluation (perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e381d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98433061943544b8850f17ab11406ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e713329fc446bfaa47cd2b8fd7d3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': 'mps', 'source_lstm': 'file:test.txt', 'lstm_ppl': 627.7102574173653, 'lstm_avg_loss': 6.442078686463452, 'lstm_tokens': 407, 'source_trans': 'file:test.txt', 'trans_ppl': 149.8652125502354, 'trans_avg_loss': 5.0097363071301055, 'trans_tokens': 407}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both final models on the dataset test split (fallback to test.txt)\n",
    "# Load the best available weights if final paths exist, else use in-memory models\n",
    "try:\n",
    "    lstm.load_state_dict(torch.load(\"outputs/lstm/final.pt\", map_location=\"cpu\"))\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    trans.load_state_dict(torch.load(\"outputs/transformer/final.pt\", map_location=\"cpu\"))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Select best available device (CUDA > MPS > CPU)\n",
    "device = best_device()\n",
    "print(\"Using device:\", device)\n",
    "lstm.to(device).eval()\n",
    "trans.to(device).eval()\n",
    "\n",
    "lstm_ppl, lstm_loss, lstm_tokens, src1 = evaluate_on_hf_or_file(\n",
    "    lstm, tokenizer, hf_split=\"test\", fallback_path=\"test.txt\", max_examples=5000, batch_size=16, max_length=256\n",
    ")\n",
    "trans_ppl, trans_loss, trans_tokens, src2 = evaluate_on_hf_or_file(\n",
    "    trans, tokenizer, hf_split=\"test\", fallback_path=\"test.txt\", max_examples=5000, batch_size=16, max_length=256\n",
    ")\n",
    "\n",
    "print({\n",
    "    \"device\": str(device),\n",
    "    \"source_lstm\": src1,\n",
    "    \"lstm_ppl\": lstm_ppl,\n",
    "    \"lstm_avg_loss\": lstm_loss,\n",
    "    \"lstm_tokens\": lstm_tokens,\n",
    "    \"source_trans\": src2,\n",
    "    \"trans_ppl\": trans_ppl,\n",
    "    \"trans_avg_loss\": trans_loss,\n",
    "    \"trans_tokens\": trans_tokens,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lingwistyka_obliczeniowa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
