{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81f4e8f",
   "metadata": {},
   "source": [
    "# Lab 4: Memory-Efficient Transformer Training Techniques\n",
    "\n",
    "This notebook compares several modern memory-optimization techniques used during Transformer training:\n",
    "1. **Baseline** with TF32 + FP32 full precision\n",
    "2. **BF16 Automatic Mixed Precision** \n",
    "3. **FlashAttention** (FlashAttention 2)\n",
    "4. **Windowed (Local) Attention**\n",
    "5. **Gradient Checkpointing**\n",
    "\n",
    "For each technique, we measure:\n",
    "- GPU memory usage\n",
    "- Maximum batch size that fits into memory\n",
    "- Training speed (time per step and total time for 1 epoch)\n",
    "- Final model performance (perplexity after 1 epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a414f",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86296efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Memory: 25.26 GB\n",
      "Currently allocated: 4.37 GB\n",
      "Currently reserved: 7.71 GB\n",
      "CUDA test: OK\n",
      "\n",
      "Using device: cuda\n",
      "FlashAttention available: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggressively clear any leftover CUDA memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Import from modules\n",
    "from modules import (\n",
    "    DEVICE,\n",
    "    TransformerConfig,\n",
    "    TrainingConfig,\n",
    "    TransformerLanguageModel,\n",
    "    build_tokenizer,\n",
    "    run_experiment,\n",
    "    count_parameters,\n",
    "    clear_cuda_memory,\n",
    ")\n",
    "from modules.transformer import FLASH_ATTN_AVAILABLE\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU Memory: {gpu_mem:.2f} GB\")\n",
    "    print(f\"Currently allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "    print(f\"Currently reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
    "    # Test CUDA works\n",
    "    try:\n",
    "        test_tensor = torch.zeros(1, device=\"cuda\")\n",
    "        del test_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"CUDA test: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"CUDA test failed: {e}\")\n",
    "print(f\"\\nUsing device: {DEVICE}\")\n",
    "print(f\"FlashAttention available: {FLASH_ATTN_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bcee2c",
   "metadata": {},
   "source": [
    "## 2. Initialize Tokenizer and Results Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495f70bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50257\n",
      "Pad token ID: 50256\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = build_tokenizer(\"gpt2\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Pad token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Store all experiment results\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb613bc9",
   "metadata": {},
   "source": [
    "## 3. Experiment 0: Baseline (TF32/FP32 Full Precision)\n",
    "\n",
    "This is the baseline experiment with full precision training using TF32 for matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef86d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment: Baseline (TF32/FP32)\n",
      "============================================================\n",
      "\n",
      "Finding maximum batch size...\n",
      "  Batch size 2: OK (Peak: 0.56 GB)\n",
      "  Batch size 4: OK (Peak: 1.06 GB)\n",
      "  Batch size 8: OK (Peak: 2.03 GB)\n",
      "  Batch size 16: OK (Peak: 3.98 GB)\n",
      "  Batch size 32: OK (Peak: 7.87 GB)\n",
      "  Batch size 64: OK (Peak: 15.67 GB)\n",
      "  Batch size 128: OOM\n",
      "Maximum batch size: 64\n",
      "\n",
      "Model parameters: 16,021,248\n",
      "Batch size: 64\n",
      "BF16: False\n",
      "Flash Attention: False\n",
      "Windowed Attention: False\n",
      "Gradient Checkpointing: False\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b522fbfcf86141c3898369eeb8948ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23878d4969a14cf3b9bd95bb2a4c2fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating perplexity on validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8983908853774ceaa603a66ab61e2cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Baseline (TF32/FP32):\n",
      "  name: Baseline (TF32/FP32)\n",
      "  max_batch_size: 64\n",
      "  avg_loss: 8.6448\n",
      "  perplexity: 5680.2949\n",
      "  avg_step_time: 0.0795\n",
      "  total_time: 16.0645\n",
      "  steps: 200\n",
      "  forward_memory_gb: 3.4410\n",
      "  backward_memory_gb: 3.4410\n",
      "  peak_memory_gb: 15.7940\n",
      "  val_perplexity: 1083.6308\n",
      "  val_loss: 6.9881\n"
     ]
    }
   ],
   "source": [
    "# Experiment 0: Baseline with TF32/FP32\n",
    "baseline_model_config = TransformerConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_dim=256,\n",
    "    n_heads=8,\n",
    "    n_layers=4,\n",
    "    ff_dim=1024,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=512,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    use_flash_attention=False,\n",
    "    use_windowed_attention=False,\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "baseline_train_config = TrainingConfig(\n",
    "    batch_size=16,  # Will be adjusted by find_max_batch_size\n",
    "    max_length=256,\n",
    "    steps_per_epoch=200,\n",
    "    num_epochs=1,\n",
    "    lr=3e-4,\n",
    "    warmup_steps=200,\n",
    "    grad_clip=1.0,\n",
    "    use_bf16=False,\n",
    "    use_flash_attention=False,\n",
    "    use_windowed_attention=False,\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "baseline_results = run_experiment(\n",
    "    \"Baseline (TF32/FP32)\",\n",
    "    baseline_model_config,\n",
    "    baseline_train_config,\n",
    "    tokenizer,\n",
    "    find_max_bs=True,\n",
    ")\n",
    "all_results.append(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b5c0ef",
   "metadata": {},
   "source": [
    "## 4. Experiment 1: BF16 Automatic Mixed Precision\n",
    "\n",
    "Use `torch.cuda.amp.autocast(dtype=torch.bfloat16)` for mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d11d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment: BF16 Mixed Precision\n",
      "============================================================\n",
      "\n",
      "Finding maximum batch size...\n",
      "  Batch size 2: OK (Peak: 4.83 GB)\n",
      "  Batch size 4: OK (Peak: 5.19 GB)\n",
      "  Batch size 8: OK (Peak: 5.91 GB)\n",
      "  Batch size 16: OK (Peak: 7.35 GB)\n",
      "  Batch size 32: OK (Peak: 10.24 GB)\n",
      "  Batch size 64: OK (Peak: 16.01 GB)\n",
      "  Batch size 128: OOM\n",
      "Maximum batch size: 64\n",
      "\n",
      "Model parameters: 16,021,248\n",
      "Batch size: 64\n",
      "BF16: True\n",
      "Flash Attention: False\n",
      "Windowed Attention: False\n",
      "Gradient Checkpointing: False\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46200913f1734d7fb757b35662b2bdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e76f2e8190448598fbe076af7e290f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating perplexity on validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e790f70a914aafad25d068e4bd0a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/modules/eval.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=torch.bfloat16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for BF16 Mixed Precision:\n",
      "  name: BF16 Mixed Precision\n",
      "  max_batch_size: 64\n",
      "  avg_loss: 8.6508\n",
      "  perplexity: 5714.9407\n",
      "  avg_step_time: 0.0606\n",
      "  total_time: 12.1390\n",
      "  steps: 200\n",
      "  forward_memory_gb: 6.1460\n",
      "  backward_memory_gb: 6.1460\n",
      "  peak_memory_gb: 16.1360\n",
      "  val_perplexity: 1082.1886\n",
      "  val_loss: 6.9867\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: BF16 Mixed Precision\n",
    "bf16_model_config = TransformerConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_dim=256,\n",
    "    n_heads=8,\n",
    "    n_layers=4,\n",
    "    ff_dim=1024,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=512,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    use_flash_attention=False,\n",
    "    use_windowed_attention=False,\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "bf16_train_config = TrainingConfig(\n",
    "    batch_size=16,  # Will be adjusted by find_max_batch_size\n",
    "    max_length=256,\n",
    "    steps_per_epoch=200,\n",
    "    num_epochs=1,\n",
    "    lr=3e-4,\n",
    "    warmup_steps=200,\n",
    "    grad_clip=1.0,\n",
    "    use_bf16=True,  # Enable BF16\n",
    "    use_flash_attention=False,\n",
    "    use_windowed_attention=False,\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "bf16_results = run_experiment(\n",
    "    \"BF16 Mixed Precision\",\n",
    "    bf16_model_config,\n",
    "    bf16_train_config,\n",
    "    tokenizer,\n",
    "    find_max_bs=True,\n",
    ")\n",
    "all_results.append(bf16_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e545f4",
   "metadata": {},
   "source": [
    "## 5. Experiment 2: FlashAttention\n",
    "\n",
    "Replace the default attention mechanism with FlashAttention 2 for memory-efficient attention computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a337bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment: FlashAttention + BF16\n",
      "============================================================\n",
      "\n",
      "Finding maximum batch size...\n",
      "  Batch size 2: OK (Peak: 4.80 GB)\n",
      "  Batch size 4: OK (Peak: 5.13 GB)\n",
      "  Batch size 8: OK (Peak: 5.79 GB)\n",
      "  Batch size 16: OK (Peak: 7.12 GB)\n",
      "  Batch size 32: OK (Peak: 9.77 GB)\n",
      "  Batch size 64: OK (Peak: 15.07 GB)\n",
      "  Batch size 128: OOM\n",
      "Maximum batch size: 64\n",
      "\n",
      "Model parameters: 16,021,248\n",
      "Batch size: 64\n",
      "BF16: True\n",
      "Flash Attention: True\n",
      "Windowed Attention: False\n",
      "Gradient Checkpointing: False\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9157691a92b34a21a28c3c3fb1370747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9dfa7e4d0c4890bb22f0435668dc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating perplexity on validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca847fed4864b85a064f93c9ab5f8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for FlashAttention + BF16:\n",
      "  name: FlashAttention + BF16\n",
      "  max_batch_size: 64\n",
      "  avg_loss: 8.6575\n",
      "  perplexity: 5753.1768\n",
      "  avg_step_time: 0.0436\n",
      "  total_time: 8.8873\n",
      "  steps: 200\n",
      "  forward_memory_gb: 6.1460\n",
      "  backward_memory_gb: 6.1460\n",
      "  peak_memory_gb: 15.1980\n",
      "  val_perplexity: 1083.4000\n",
      "  val_loss: 6.9879\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: FlashAttention (with BF16 - required for FlashAttention)\n",
    "if FLASH_ATTN_AVAILABLE:\n",
    "    flash_model_config = TransformerConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        emb_dim=256,\n",
    "        n_heads=8,\n",
    "        n_layers=4,\n",
    "        ff_dim=1024,\n",
    "        dropout=0.1,\n",
    "        max_seq_len=512,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_flash_attention=True,  # Enable FlashAttention\n",
    "        use_windowed_attention=False,\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    flash_train_config = TrainingConfig(\n",
    "        batch_size=16,  # Will be adjusted by find_max_batch_size\n",
    "        max_length=256,\n",
    "        steps_per_epoch=200,\n",
    "        num_epochs=1,\n",
    "        lr=3e-4,\n",
    "        warmup_steps=200,\n",
    "        grad_clip=1.0,\n",
    "        use_bf16=True,  # FlashAttention requires BF16/FP16\n",
    "        use_flash_attention=True,\n",
    "        use_windowed_attention=False,\n",
    "        gradient_checkpointing=False,\n",
    "    )\n",
    "\n",
    "    flash_results = run_experiment(\n",
    "        \"FlashAttention + BF16\",\n",
    "        flash_model_config,\n",
    "        flash_train_config,\n",
    "        tokenizer,\n",
    "        find_max_bs=True,\n",
    "    )\n",
    "    all_results.append(flash_results)\n",
    "else:\n",
    "    print(\"FlashAttention not available. Skipping this experiment.\")\n",
    "    print(\"To install FlashAttention, run: pip install flash-attn --no-build-isolation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0da3b",
   "metadata": {},
   "source": [
    "## 6. Experiment 3: Windowed (Local) Attention\n",
    "\n",
    "Replace full self-attention with sliding-window attention to reduce memory complexity from O(n²) to O(n × window_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3c61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment: Windowed Attention (w=128) + FlashAttn\n",
      "============================================================\n",
      "\n",
      "Finding maximum batch size...\n",
      "  Batch size 2: OK (Peak: 4.80 GB)\n",
      "  Batch size 4: OK (Peak: 5.13 GB)\n",
      "  Batch size 8: OK (Peak: 5.79 GB)\n",
      "  Batch size 16: OK (Peak: 7.12 GB)\n",
      "  Batch size 32: OK (Peak: 9.77 GB)\n",
      "  Batch size 64: OK (Peak: 15.07 GB)\n",
      "  Batch size 128: OOM\n",
      "Maximum batch size: 64\n",
      "\n",
      "Model parameters: 16,021,248\n",
      "Batch size: 64\n",
      "BF16: True\n",
      "Flash Attention: True\n",
      "Windowed Attention: True\n",
      "Gradient Checkpointing: False\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ea84a73098476c94bfe2b2bebd92d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171b5bfcfb484385be35eccceee51878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating perplexity on validation set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e21af3fa738471eb9f743e1c8b6ef93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment 3: Windowed Attention (using FlashAttention's sliding window if available)\n",
    "window_model_config = TransformerConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_dim=256,\n",
    "    n_heads=8,\n",
    "    n_layers=4,\n",
    "    ff_dim=1024,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=512,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    use_flash_attention=FLASH_ATTN_AVAILABLE,  # Use FlashAttention for windowed if available\n",
    "    use_windowed_attention=True,  # Enable windowed attention\n",
    "    window_size=128,  # Sliding window size\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "window_train_config = TrainingConfig(\n",
    "    batch_size=16,  # Will be adjusted by find_max_batch_size\n",
    "    max_length=256,\n",
    "    steps_per_epoch=200,\n",
    "    num_epochs=1,\n",
    "    lr=3e-4,\n",
    "    warmup_steps=200,\n",
    "    grad_clip=1.0,\n",
    "    use_bf16=FLASH_ATTN_AVAILABLE,  # BF16 required for FlashAttention\n",
    "    use_flash_attention=FLASH_ATTN_AVAILABLE,\n",
    "    use_windowed_attention=True,\n",
    "    window_size=128,\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "window_results = run_experiment(\n",
    "    \"Windowed Attention (w=128)\" + (\" + FlashAttn\" if FLASH_ATTN_AVAILABLE else \"\"),\n",
    "    window_model_config,\n",
    "    window_train_config,\n",
    "    tokenizer,\n",
    "    find_max_bs=True,\n",
    ")\n",
    "all_results.append(window_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f02556",
   "metadata": {},
   "source": [
    "## 7. Experiment 4: Gradient Checkpointing\n",
    "\n",
    "Enable gradient checkpointing to trade compute for memory by recomputing activations during backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Gradient Checkpointing (with BF16)\n",
    "gc_model_config = TransformerConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    emb_dim=256,\n",
    "    n_heads=8,\n",
    "    n_layers=4,\n",
    "    ff_dim=1024,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=512,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    use_flash_attention=False,\n",
    "    use_windowed_attention=False,\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    ")\n",
    "\n",
    "gc_train_config = TrainingConfig(\n",
    "    batch_size=16,  # Will be adjusted by find_max_batch_size\n",
    "    max_length=256,\n",
    "    steps_per_epoch=200,\n",
    "    num_epochs=1,\n",
    "    lr=3e-4,\n",
    "    warmup_steps=200,\n",
    "    grad_clip=1.0,\n",
    "    use_bf16=True,  # Also use BF16 for fair comparison\n",
    "    use_flash_attention=False,\n",
    "    use_windowed_attention=False,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "gc_results = run_experiment(\n",
    "    \"Gradient Checkpointing + BF16\",\n",
    "    gc_model_config,\n",
    "    gc_train_config,\n",
    "    tokenizer,\n",
    "    find_max_bs=True,\n",
    ")\n",
    "all_results.append(gc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a93b9",
   "metadata": {},
   "source": [
    "## 8. Results Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3408046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Select key columns for display\n",
    "display_cols = [\n",
    "    'name', \n",
    "    'max_batch_size', \n",
    "    'peak_memory_gb', \n",
    "    'avg_step_time', \n",
    "    'total_time',\n",
    "    'val_perplexity',\n",
    "    'avg_loss',\n",
    "]\n",
    "\n",
    "# Filter to only existing columns\n",
    "display_cols = [c for c in display_cols if c in results_df.columns]\n",
    "summary_df = results_df[display_cols].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary_df.columns = [\n",
    "    'Technique',\n",
    "    'Max Batch Size',\n",
    "    'Peak Memory (GB)',\n",
    "    'Step Time (s)',\n",
    "    'Total Time (s)',\n",
    "    'Validation Perplexity',\n",
    "    'Training Loss',\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results to JSON\n",
    "results_path = \"outputs/summary.json\"\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(f\"\\nResults saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01902240",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e969e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Peak Memory Usage\n",
    "ax1 = axes[0, 0]\n",
    "names = [r['name'] for r in all_results]\n",
    "peak_mem = [r.get('peak_memory_gb', 0) for r in all_results]\n",
    "bars = ax1.bar(range(len(names)), peak_mem, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'][:len(names)])\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Peak Memory (GB)')\n",
    "ax1.set_title('Peak GPU Memory Usage')\n",
    "for bar, val in zip(bars, peak_mem):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{val:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: Maximum Batch Size\n",
    "ax2 = axes[0, 1]\n",
    "max_bs = [r.get('max_batch_size', 0) for r in all_results]\n",
    "bars = ax2.bar(range(len(names)), max_bs, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'][:len(names)])\n",
    "ax2.set_xticks(range(len(names)))\n",
    "ax2.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Max Batch Size')\n",
    "ax2.set_title('Maximum Batch Size Fitting in Memory')\n",
    "for bar, val in zip(bars, max_bs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, str(val), ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Step Time\n",
    "ax3 = axes[1, 0]\n",
    "step_times = [r.get('avg_step_time', 0) for r in all_results]\n",
    "bars = ax3.bar(range(len(names)), step_times, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'][:len(names)])\n",
    "ax3.set_xticks(range(len(names)))\n",
    "ax3.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax3.set_ylabel('Step Time (s)')\n",
    "ax3.set_title('Average Step Time')\n",
    "for bar, val in zip(bars, step_times):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 4: Validation Perplexity\n",
    "ax4 = axes[1, 1]\n",
    "val_ppl = [r.get('val_perplexity', 0) for r in all_results]\n",
    "bars = ax4.bar(range(len(names)), val_ppl, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'][:len(names)])\n",
    "ax4.set_xticks(range(len(names)))\n",
    "ax4.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax4.set_ylabel('Perplexity')\n",
    "ax4.set_title('Validation Perplexity After 1 Epoch')\n",
    "for bar, val in zip(bars, val_ppl):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/comparison_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlots saved to outputs/comparison_plots.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4553f1a",
   "metadata": {},
   "source": [
    "## 10. Analysis and Conclusions\n",
    "\n",
    "### Memory Efficiency\n",
    "- **BF16 Mixed Precision** reduces memory by storing weights and activations in 16-bit format instead of 32-bit\n",
    "- **FlashAttention** avoids materializing the full attention matrix, reducing memory from O(n²) to O(n)\n",
    "- **Windowed Attention** limits attention to local context, further reducing memory requirements\n",
    "- **Gradient Checkpointing** trades compute for memory by not storing intermediate activations\n",
    "\n",
    "### Speed Trade-offs\n",
    "- **BF16** is generally faster due to reduced memory bandwidth and Tensor Core utilization\n",
    "- **FlashAttention** is optimized for modern GPUs and often faster than standard attention\n",
    "- **Windowed Attention** can be faster for long sequences but may hurt model quality\n",
    "- **Gradient Checkpointing** increases compute time (~20-30%) due to recomputation\n",
    "\n",
    "### Perplexity Impact\n",
    "- **BF16 and FlashAttention** should have minimal impact on final perplexity\n",
    "- **Windowed Attention** may degrade perplexity as it limits the model's ability to attend to distant tokens\n",
    "- **Gradient Checkpointing** should have no impact on perplexity (mathematically equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee94f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    baseline = all_results[0]\n",
    "    \n",
    "    print(f\"\\nBaseline ({baseline['name']}):\")\n",
    "    print(f\"  Peak Memory: {baseline.get('peak_memory_gb', 'N/A')} GB\")\n",
    "    print(f\"  Max Batch Size: {baseline.get('max_batch_size', 'N/A')}\")\n",
    "    print(f\"  Step Time: {baseline.get('avg_step_time', 'N/A'):.4f}s\")\n",
    "    print(f\"  Val Perplexity: {baseline.get('val_perplexity', 'N/A'):.2f}\")\n",
    "    \n",
    "    print(\"\\nComparison to Baseline:\")\n",
    "    for result in all_results[1:]:\n",
    "        name = result['name']\n",
    "        print(f\"\\n{name}:\")\n",
    "        \n",
    "        # Memory reduction\n",
    "        if baseline.get('peak_memory_gb') and result.get('peak_memory_gb'):\n",
    "            mem_reduction = (1 - result['peak_memory_gb'] / baseline['peak_memory_gb']) * 100\n",
    "            print(f\"  Memory: {result['peak_memory_gb']:.2f} GB ({mem_reduction:+.1f}%)\")\n",
    "        \n",
    "        # Batch size increase\n",
    "        if baseline.get('max_batch_size') and result.get('max_batch_size'):\n",
    "            bs_increase = (result['max_batch_size'] / baseline['max_batch_size'] - 1) * 100\n",
    "            print(f\"  Max Batch Size: {result['max_batch_size']} ({bs_increase:+.1f}%)\")\n",
    "        \n",
    "        # Speed comparison\n",
    "        if baseline.get('avg_step_time') and result.get('avg_step_time'):\n",
    "            speed_change = (1 - result['avg_step_time'] / baseline['avg_step_time']) * 100\n",
    "            print(f\"  Step Time: {result['avg_step_time']:.4f}s ({speed_change:+.1f}%)\")\n",
    "        \n",
    "        # Perplexity comparison\n",
    "        if baseline.get('val_perplexity') and result.get('val_perplexity'):\n",
    "            ppl_change = result['val_perplexity'] - baseline['val_perplexity']\n",
    "            print(f\"  Val Perplexity: {result['val_perplexity']:.2f} ({ppl_change:+.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
