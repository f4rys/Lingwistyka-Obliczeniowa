{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5422a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "ROOT = Path(\".\").resolve()\n",
    "OUTPUT_DIR = ROOT / \"outputs\"\n",
    "ANNOTATIONS_DIR = ROOT / \"annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest annotation file\n",
    "annotation_files = sorted(ANNOTATIONS_DIR.glob('annotations_lab5_experiment_*.csv'))\n",
    "latest_anno = annotation_files[-1]\n",
    "df = pd.read_csv(latest_anno)\n",
    "\n",
    "# Filter out un-scored rows if any\n",
    "df = df[df['score'].notna()].copy()\n",
    "df['score'] = df['score'].astype(float)\n",
    "df['normalized_score'] = df['score'] / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b339e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics\n",
    "agg_metrics = df.groupby(['task_id', 'task_name', 'model', 'strategy']).agg(\n",
    "    avg_score=('normalized_score', 'mean'),\n",
    "    n=('score', 'size')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the impact of prompting strategies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=agg_metrics, x='strategy', y='avg_score', hue='model')\n",
    "plt.title('Impact of Prompting Strategy (Manual Score) across Models')\n",
    "plt.ylabel('Average Score (Normalized 0-1)')\n",
    "plt.xlabel('Prompting Strategy')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"strategy_comparison_manual.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by Task Type\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=agg_metrics, x='task_name', y='avg_score', hue='model')\n",
    "plt.title('Model Performance by Task Type (Manual Score)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Average Score (Normalized 0-1)')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"task_performance_manual.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
