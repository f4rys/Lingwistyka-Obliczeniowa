{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a485e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04240cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "ROOT = Path(\".\").resolve()\n",
    "OUTPUT_DIR = ROOT / \"outputs\"\n",
    "TASKS_DIR = ROOT / \"tasks\"\n",
    "ANNOTATIONS_DIR = ROOT / \"annotations\"\n",
    "ANNOTATIONS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785234d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Task details for display\n",
    "task_files = sorted(TASKS_DIR.glob('*.json'))\n",
    "TASKS = {}\n",
    "for f in task_files:\n",
    "    d = json.loads(f.read_text(encoding='utf-8'))\n",
    "    TASKS[d['id']] = d\n",
    "\n",
    "# Find the latest results file\n",
    "result_files = sorted(OUTPUT_DIR.glob('lab5_experiment_*.csv'))\n",
    "latest_file = result_files[-1]\n",
    "df = pd.read_csv(latest_file)\n",
    "\n",
    "# If an annotation file exists for this run, we resume\n",
    "annotation_file = ANNOTATIONS_DIR / f\"annotations_{latest_file.name}\"\n",
    "\n",
    "if annotation_file.exists():\n",
    "    print(f\"Resuming from existing annotations: {annotation_file.name}\")\n",
    "    df_annotated = pd.read_csv(annotation_file)\n",
    "else:\n",
    "    df_annotated = df.copy()\n",
    "    if 'score' not in df_annotated.columns:\n",
    "        df_annotated['score'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_manual_scoring(df_to_score, task_map):\n",
    "    \"\"\"Anonymized scoring loop.\"\"\"\n",
    "    # We only score successful runs that haven't been scored yet (score is None or NaN)\n",
    "    indices = df_to_score[df_to_score['success'] & df_to_score['score'].isna()].index.tolist()\n",
    "    \n",
    "    if not indices:\n",
    "        print(\"All entries have been scored!\")\n",
    "        return\n",
    "\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    print(f\"Total entries to score: {len(indices)}\")\n",
    "    print(\"Scoring Rules: 0 (poor) to 5 (excellent). Type 'q' to quit and save.\")\n",
    "    \n",
    "    count = 0\n",
    "    total = len(indices)\n",
    "    try:\n",
    "        for idx in indices:\n",
    "            row = df_to_score.loc[idx]\n",
    "            task = task_map.get(row['task_id'], {})\n",
    "            \n",
    "            # Extract plain input from the task data to hide prompting strategy\n",
    "            input_text = task.get('eval_example', {}).get('input', 'N/A')\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"Progress: {count+1}/{total}\")\n",
    "            print(f\"TASK: {task.get('name', 'N/A')}\")\n",
    "            print(f\"Description: {task.get('description', 'N/A')}\")\n",
    "            print(f\"Criteria: {task.get('eval_criteria', 'N/A')}\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"INPUT: {input_text}\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"MODEL RESPONSE:\\n{row['response']}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            valid_input = False\n",
    "            while not valid_input:\n",
    "                val = input(\"Score (0-5) or 'q': \").strip().lower()\n",
    "                if val == 'q':\n",
    "                    print(\"\\nQuitting...\")\n",
    "                    return\n",
    "                try:\n",
    "                    score = float(val)\n",
    "                    if 0 <= score <= 5:\n",
    "                        df_to_score.at[idx, 'score'] = score\n",
    "                        valid_input = True\n",
    "                        count += 1\n",
    "                        # Save progress after each entry to be safe\n",
    "                        df_to_score.to_csv(annotation_file, index=False)\n",
    "                    else:\n",
    "                        print(\"Please enter a number between 0 and 5.\")\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Enter 0-5 or 'q'.\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted. Progress saved.\")\n",
    "\n",
    "run_manual_scoring(df_annotated, TASKS)\n",
    "print(f\"\\nFinal annotations saved to {annotation_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
