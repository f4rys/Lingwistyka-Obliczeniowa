{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ed9cfd",
   "metadata": {},
   "source": [
    "# Lab 5: Stage 2 â€” Blind Evaluation\n",
    "This notebook implements an **anonymized scoring system** to evaluate model outputs objectively. \n",
    "\n",
    "### Objectives:\n",
    "- **Blind Review**: Hide model identities and prompting strategies from the evaluator to prevent bias.\n",
    "- **Progressive Saving**: Automatically save progress to the `annotations/` directory after each entry.\n",
    "- **Randomized Order**: Present responses in a random order to avoid sequence bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a485e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04240cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory configuration\n",
    "ROOT = Path(\".\").resolve()\n",
    "OUTPUT_DIR = ROOT / \"outputs\"\n",
    "TASKS_DIR = ROOT / \"tasks\"\n",
    "ANNOTATIONS_DIR = ROOT / \"annotations\"\n",
    "ANNOTATIONS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785234d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Task details for display\n",
    "task_files = sorted(TASKS_DIR.glob('*.json'))\n",
    "TASKS = {}\n",
    "for f in task_files:\n",
    "    d = json.loads(f.read_text(encoding='utf-8'))\n",
    "    TASKS[d['id']] = d\n",
    "\n",
    "# Find the latest results file\n",
    "result_files = sorted(OUTPUT_DIR.glob('lab5_experiment_*.csv'))\n",
    "latest_file = result_files[-1]\n",
    "df = pd.read_csv(latest_file)\n",
    "\n",
    "# If an annotation file exists for this run, we resume\n",
    "annotation_file = ANNOTATIONS_DIR / f\"annotations_{latest_file.name}\"\n",
    "\n",
    "if annotation_file.exists():\n",
    "    print(f\"Resuming from existing annotations: {annotation_file.name}\")\n",
    "    df_annotated = pd.read_csv(annotation_file)\n",
    "else:\n",
    "    df_annotated = df.copy()\n",
    "    if 'score' not in df_annotated.columns:\n",
    "        df_annotated['score'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f637a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring complete! All selected items have been processed.\n",
      "\n",
      "Annotations are stored in: /Users/wojciechbartoszek/Documents/studia/lingwistyka_obliczeniowa/Lingwistyka-Obliczeniowa/lab5/annotations/annotations_lab5_experiment_20260111_115522.csv\n"
     ]
    }
   ],
   "source": [
    "def run_manual_scoring(df_to_score, task_map):\n",
    "    \"\"\"\n",
    "    Interactive loop for blind scoring of model responses.\n",
    "    Filters for successful, un-scored entries.\n",
    "    \"\"\"\n",
    "    indices = df_to_score[df_to_score['success'] & df_to_score['score'].isna()].index.tolist()\n",
    "    \n",
    "    if not indices:\n",
    "        print(\"Scoring complete. All items have been processed.\")\n",
    "        return\n",
    "\n",
    "    random.shuffle(indices) # Ensure evaluation order is random\n",
    "    total = len(indices)\n",
    "\n",
    "    # Initial clear to clean up setup messages\n",
    "    clear_output(wait=False)\n",
    "    \n",
    "    try:\n",
    "        for count, idx in enumerate(indices, 1):\n",
    "            row = df_to_score.loc[idx]\n",
    "            task = task_map.get(row['task_id'], {})\n",
    "            \n",
    "            # Display only the original input to maintain blindness to the strategy\n",
    "            input_text = task.get('eval_example', {}).get('input', 'N/A')\n",
    "            \n",
    "            # Context and instructions (always shown inside the loop)\n",
    "            print(\"=\"*80)\n",
    "            print(f\"Progress: {count}/{total}\")\n",
    "            print(f\"Scale: 0 (Fail) to 5 (Perfect). 'q' to Save & Exit.\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"TASK: {task.get('name', 'N/A')}\")\n",
    "            print(f\"Criteria: {task.get('eval_criteria', 'N/A')}\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"INPUT: {input_text}\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"MODEL RESPONSE:\\n{row['response']}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            valid_input = False\n",
    "            while not valid_input:\n",
    "                val = input(\"Score (0-5) or 'q': \").strip().lower()\n",
    "                if val == 'q':\n",
    "                    print(\"\\nSession paused. Progress saved.\")\n",
    "                    return\n",
    "                try:\n",
    "                    score = float(val)\n",
    "                    if 0 <= score <= 5:\n",
    "                        df_to_score.at[idx, 'score'] = score\n",
    "                        valid_input = True\n",
    "                        # Immediate persistent save\n",
    "                        df_to_score.to_csv(annotation_file, index=False)\n",
    "                    else:\n",
    "                        print(\"Error: Score must be between 0 and 5.\")\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Use 0-5 or 'q'.\")\n",
    "            \n",
    "            # Clear and wait for the next iteration's content\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        print(\"Scoring complete! All selected items have been processed.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted. Progress saved.\")\n",
    "\n",
    "# Launch scoring session\n",
    "run_manual_scoring(df_annotated, TASKS)\n",
    "print(f\"\\nAnnotations are stored in: {annotation_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp-ling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
