{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fb8d41",
   "metadata": {},
   "source": [
    "# Lab 5 â€” Evaluating LLMs with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "from ollama import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from eval_utils import (\n",
    "    validate_models,\n",
    "    should_skip_cot,\n",
    "    preview_experiments,\n",
    "    zero_shot_prompt,\n",
    "    few_shot_prompt,\n",
    "    cot_prompt,\n",
    "    exact_match,\n",
    "    query_chat,\n",
    "    query_generate,\n",
    "    save_prompt_run,\n",
    "    run_evaluation,\n",
    "    token_overlap,\n",
    "    compute_metrics,\n",
    "    get_judge_score,\n",
    "    run_judge_evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04625f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "ROOT = Path(\".\").resolve()\n",
    "LAB_DIR = ROOT\n",
    "OUTPUT_DIR = LAB_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TASKS_DIR = LAB_DIR / 'tasks'\n",
    "TASKS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client (defaults to http://localhost:11434)\n",
    "client = Client()\n",
    "models_resp = client.list()\n",
    "\n",
    "print(\"Available models:\")\n",
    "for m in models_resp.models:\n",
    "    print(\" -\", m.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-task JSON files from `lab5/tasks/` (no default task definitions in notebook)\n",
    "task_files = sorted(TASKS_DIR.glob('*.json'))\n",
    "\n",
    "# Load tasks\n",
    "TASKS = []\n",
    "DEV_EXAMPLES = {}\n",
    "EVAL_EXAMPLES = {}\n",
    "\n",
    "for f in task_files:\n",
    "    d = json.loads(f.read_text(encoding='utf-8'))\n",
    "    TASKS.append({'id': d['id'], 'name': d['name'], 'description': d.get('description',''), 'eval_criteria': d.get('eval_criteria','')})\n",
    "    DEV_EXAMPLES[d['id']] = d.get('dev_examples', [])\n",
    "    EVAL_EXAMPLES[d['id']] = d.get('eval_example', {})\n",
    "\n",
    "print(f\"Loaded {len(TASKS)} tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6708ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime querying helpers are provided by `lab5/eval_utils.py` and imported into the notebook.\n",
    "# Functions available: `query_chat`, `query_generate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df64b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime experiment loop helpers are provided by `lab5/eval_utils.py` and imported into the notebook.\n",
    "# Use `run_evaluation(models, strategies, tasks, examples, client, output_dir)` to run experiments and persist outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7efe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics helpers are provided by `lab5/eval_utils.py` and imported into the notebook.\n",
    "# Functions available: `token_overlap`, `compute_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eda5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and strategies for the experiment\n",
    "selected_models = [\n",
    "    'SpeakLeash/bielik-1.5b-v3.0-instruct:Q8_0',\n",
    "    'ministral-3:3b',\n",
    "    'deepseek-r1:7b'\n",
    "]\n",
    "strategies = ['zero', 'few', 'cot']\n",
    "\n",
    "# Execute the evaluation\n",
    "# This will iterate through 10 tasks, 3 models, and up to 3 strategies per model.\n",
    "# Total expected runs: 10 * (2*3 + 1*2) = 10 * 8 = 80 runs.\n",
    "results = run_evaluation(\n",
    "    models=selected_models,\n",
    "    strategies=strategies,\n",
    "    tasks=TASKS,\n",
    "    examples=EVAL_EXAMPLES,\n",
    "    client=client,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_prefix='lab5_experiment',\n",
    "    dev_examples=DEV_EXAMPLES,\n",
    "    tasks_dir=TASKS_DIR\n",
    ")\n",
    "\n",
    "# Compute and display metrics\n",
    "# `compute_metrics` will add 'expected', 'exact_match' and 'overlap_score' and return aggregated metrics\n",
    "df_results, agg_metrics = compute_metrics(results, EVAL_EXAMPLES)\n",
    "\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "# Pivot the results for a better view: Models as columns, Tasks as rows\n",
    "summary_pivot = agg_metrics.pivot_table(\n",
    "    index=['task_id', 'task_name'], \n",
    "    columns=['model', 'strategy'], \n",
    "    values='accuracy'\n",
    ")\n",
    "display(summary_pivot)\n",
    "\n",
    "# Save the final aggregated metrics to a file for the report\n",
    "agg_metrics.to_csv(OUTPUT_DIR / \"final_experiment_results.csv\", index=False)\n",
    "print(f\"\\nFinal results saved to {OUTPUT_DIR / 'final_experiment_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c6b57",
   "metadata": {},
   "source": [
    "### LLM-as-a-Judge Evaluation\n",
    "Since `exact_match` is too strict for open-ended tasks, we use a reasoning model (`deepseek-r1:7b`) to evaluate the quality of the responses based on the task criteria. \n",
    "\n",
    "The judge will assign a score from 0 to 5. We then normalize this to a 0-1 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge-evaluation helpers are provided by `lab5/eval_utils.py` and imported into the notebook.\n",
    "# Use `get_judge_score(task, prompt, response, client)` and `run_judge_evaluation(results_df, tasks, client)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the impact of prompting strategies using Judge Scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=agg_metrics, x='strategy', y='avg_judge_score', hue='model')\n",
    "plt.title('Impact of Prompting Strategy (Judge Score) across Models')\n",
    "plt.ylabel('Average Judge Score (Normalized 0-1)')\n",
    "plt.xlabel('Prompting Strategy')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"strategy_comparison_judge.png\")\n",
    "plt.show()\n",
    "\n",
    "# Performance by Task Type using Judge Scores\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=agg_metrics, x='task_name', y='avg_judge_score', hue='model')\n",
    "plt.title('Model Performance by Task Type (Judge Score)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Average Judge Score (Normalized 0-1)')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"task_performance_judge.png\")\n",
    "plt.show()\n",
    "\n",
    "# Distribution Comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "melted_metrics = agg_metrics.melt(id_vars=['model'], value_vars=['avg_accuracy', 'avg_judge_score'], \n",
    "                                   var_name='Metric', value_name='Score')\n",
    "sns.boxplot(data=melted_metrics, x='Metric', y='Score')\n",
    "plt.title('Exact Match Accuracy vs. LLM Judge Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp-ling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
