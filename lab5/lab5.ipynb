{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fb8d41",
   "metadata": {},
   "source": [
    "# Lab 5 â€” Evaluating LLMs with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from ollama import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from eval_utils import (\n",
    "    run_evaluation,\n",
    "    compute_metrics,\n",
    "    run_judge_evaluation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04625f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "ROOT = Path(\".\").resolve()\n",
    "LAB_DIR = ROOT\n",
    "OUTPUT_DIR = LAB_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TASKS_DIR = LAB_DIR / 'tasks'\n",
    "TASKS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client (defaults to http://localhost:11434)\n",
    "client = Client()\n",
    "models_resp = client.list()\n",
    "\n",
    "print(\"Available models:\")\n",
    "for m in models_resp.models:\n",
    "    print(\" -\", m.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-task JSON files from `tasks/`\n",
    "task_files = sorted(TASKS_DIR.glob('*.json'))\n",
    "\n",
    "# Load tasks\n",
    "TASKS = []\n",
    "DEV_EXAMPLES = {}\n",
    "EVAL_EXAMPLES = {}\n",
    "\n",
    "for f in task_files:\n",
    "    d = json.loads(f.read_text(encoding='utf-8'))\n",
    "    TASKS.append({'id': d['id'], 'name': d['name'], 'description': d.get('description',''), 'eval_criteria': d.get('eval_criteria','')})\n",
    "    DEV_EXAMPLES[d['id']] = d.get('dev_examples', [])\n",
    "    EVAL_EXAMPLES[d['id']] = d.get('eval_example', {})\n",
    "\n",
    "print(f\"Loaded {len(TASKS)} tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eda5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and strategies for the experiment\n",
    "selected_models = [\n",
    "    # Small model\n",
    "    'smollm:1.7b',\n",
    "    # Large model with reasoning capabilities\n",
    "    'magistral:24b'\n",
    "]\n",
    "strategies = ['zero', 'few', 'cot']\n",
    "\n",
    "# Execute the evaluation\n",
    "results = run_evaluation(\n",
    "    models=selected_models,\n",
    "    strategies=strategies,\n",
    "    tasks=TASKS,\n",
    "    examples=EVAL_EXAMPLES,\n",
    "    client=client,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_prefix='lab5_experiment',\n",
    "    dev_examples=DEV_EXAMPLES,\n",
    "    tasks_dir=TASKS_DIR\n",
    ")\n",
    "# Compute and display metrics\n",
    "df_results, agg_metrics = compute_metrics(results, EVAL_EXAMPLES)\n",
    "\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "# Pivot the results for a better view: Models as columns, Tasks as rows\n",
    "summary_pivot = agg_metrics.pivot_table(\n",
    "    index=['task_id', 'task_name'], \n",
    "    columns=['model', 'strategy'],\n",
    "    values='accuracy'\n",
    ")\n",
    "display(summary_pivot)\n",
    "\n",
    "# Save the final aggregated metrics to a file for the report\n",
    "agg_metrics.to_csv(OUTPUT_DIR / \"final_experiment_results.csv\", index=False)\n",
    "print(f\"\\nFinal results saved to {OUTPUT_DIR / 'final_experiment_results.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45417c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure judge scores are present and aggregate them for plotting\n",
    "if 'normalized_judge_score' not in df_results.columns:\n",
    "    print(\"No judge scores found. Running judge evaluation (may be slow)...\")\n",
    "    # You can change the judge_model or run this on a subset to save time\n",
    "    df_results = run_judge_evaluation(df_results, TASKS, client, judge_model='deepseek-r1:7b')\n",
    "else:\n",
    "    print(\"Using existing judge scores\")\n",
    "\n",
    "# Aggregate judge scores (normalized 0-1) per task/model/strategy\n",
    "agg_judge = df_results.groupby(['task_id', 'task_name', 'model', 'strategy']).agg(\n",
    "    avg_judge_score=('normalized_judge_score', 'mean'),\n",
    "    n_judge=('normalized_judge_score', 'size')\n",
    ").reset_index()\n",
    "\n",
    "# Merge with existing aggregated metrics\n",
    "agg_metrics = agg_metrics.merge(\n",
    "    agg_judge[['task_id', 'model', 'strategy', 'avg_judge_score']],\n",
    "    on=['task_id', 'model', 'strategy'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Ensure column names expected by plotting cells exist\n",
    "agg_metrics['avg_judge_score'] = agg_metrics['avg_judge_score'].fillna(0.0)\n",
    "agg_metrics['avg_accuracy'] = agg_metrics.get('accuracy', agg_metrics.get('avg_accuracy', None))\n",
    "if agg_metrics['avg_accuracy'].isnull().any():\n",
    "    agg_metrics['avg_accuracy'] = agg_metrics['accuracy']\n",
    "\n",
    "print('Aggregated judge scores added to `agg_metrics`.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the impact of prompting strategies using Judge Scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=agg_metrics, x='strategy', y='avg_judge_score', hue='model')\n",
    "plt.title('Impact of Prompting Strategy (Judge Score) across Models')\n",
    "plt.ylabel('Average Judge Score (Normalized 0-1)')\n",
    "plt.xlabel('Prompting Strategy')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"strategy_comparison_judge.png\")\n",
    "plt.show()\n",
    "\n",
    "# Performance by Task Type using Judge Scores\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=agg_metrics, x='task_name', y='avg_judge_score', hue='model')\n",
    "plt.title('Model Performance by Task Type (Judge Score)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Average Judge Score (Normalized 0-1)')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"task_performance_judge.png\")\n",
    "plt.show()\n",
    "\n",
    "# Distribution Comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "melted_metrics = agg_metrics.melt(id_vars=['model'], value_vars=['avg_accuracy', 'avg_judge_score'], \n",
    "                                   var_name='Metric', value_name='Score')\n",
    "sns.boxplot(data=melted_metrics, x='Metric', y='Score')\n",
    "plt.title('Exact Match Accuracy vs. LLM Judge Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp-ling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
