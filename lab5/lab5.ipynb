{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87fb8d41",
   "metadata": {},
   "source": [
    "# Lab 5 â€” Evaluating LLMs with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "from ollama import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from eval_utils import validate_models, should_skip_cot, preview_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04625f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94f1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "ROOT = Path(\".\").resolve()\n",
    "LAB_DIR = ROOT\n",
    "OUTPUT_DIR = LAB_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TASKS_DIR = LAB_DIR / 'tasks'\n",
    "TASKS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client (defaults to http://localhost:11434)\n",
    "client = Client()\n",
    "models_resp = client.list()\n",
    "\n",
    "print(\"Available models:\")\n",
    "for m in models_resp.models:\n",
    "    print(\" -\", m.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-task JSON files from `lab5/tasks/` (no default task definitions in notebook)\n",
    "task_files = sorted(TASKS_DIR.glob('*.json'))\n",
    "\n",
    "# Load tasks\n",
    "TASKS = []\n",
    "DEV_EXAMPLES = {}\n",
    "EVAL_EXAMPLES = {}\n",
    "\n",
    "for f in task_files:\n",
    "    d = json.loads(f.read_text(encoding='utf-8'))\n",
    "    TASKS.append({'id': d['id'], 'name': d['name'], 'description': d.get('description',''), 'eval_criteria': d.get('eval_criteria','')})\n",
    "    DEV_EXAMPLES[d['id']] = d.get('dev_examples', [])\n",
    "    EVAL_EXAMPLES[d['id']] = d.get('eval_example', {})\n",
    "\n",
    "print(f\"Loaded {len(TASKS)} tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt builders\n",
    "def zero_shot_prompt(task, instance=None):\n",
    "    base = f\"Task: {task['name']}\\nDescription: {task.get('description','')}\\nEvaluation criteria: {task.get('eval_criteria','')}\\n\"\n",
    "    if instance:\n",
    "        base += f\"Input: {instance}\\n\"\n",
    "    base += \"\\nPlease provide a concise, direct answer.\"\n",
    "    return base\n",
    "\n",
    "\n",
    "def few_shot_prompt(task, dev_examples, instance):\n",
    "    prompt = f\"Task: {task['name']}\\nDescription: {task.get('description','')}\\nEvaluation criteria: {task.get('eval_criteria','')}\\n\\nHere are a few examples:\\n\"\n",
    "    for ex in dev_examples:\n",
    "        prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n---\\n\"\n",
    "    prompt += f\"Now, Input: {instance}\\nPlease write Output:\" \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def cot_prompt(task, instance=None):\n",
    "    prompt = zero_shot_prompt(task, instance)\n",
    "    prompt += \"\\nLet's think step by step before answering.\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6708ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chat(model: str, prompt: str, temperature: float = 0.0, max_tokens: int = 1024, stream: bool = False):\n",
    "    \"\"\"Query a model using chat-style messages and return a dict with content and metadata.\"\"\"\n",
    "    messages = [{'role':'user', 'content': prompt}]\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = client.chat(model=model, messages=messages)\n",
    "        elapsed = time.time() - start\n",
    "        # resp is a ChatResponse; access message content\n",
    "        content = resp['message']['content'] if isinstance(resp, dict) else getattr(resp, 'message', {}).get('content', '')\n",
    "        return {\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'response': content,\n",
    "            'elapsed': elapsed,\n",
    "            'success': True,\n",
    "            'raw': resp\n",
    "        }\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'response': '',\n",
    "            'elapsed': elapsed,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "def query_generate(model: str, prompt: str, temperature: float = 0.0, max_tokens: int = 1024):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        resp = client.generate(model=model, prompt=prompt)\n",
    "        elapsed = time.time() - start\n",
    "        content = resp['message']['content'] if isinstance(resp, dict) else getattr(resp, 'message', {}).get('content', '')\n",
    "        return {\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'response': content,\n",
    "            'elapsed': elapsed,\n",
    "            'success': True,\n",
    "            'raw': resp\n",
    "        }\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start\n",
    "        return {\n",
    "            'model': model,\n",
    "            'prompt': prompt,\n",
    "            'response': '',\n",
    "            'elapsed': elapsed,\n",
    "            'success': False,\n",
    "            'error': str(e)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df64b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare helper to save prompt+response runs back to the corresponding task file (one file per category)\n",
    "def save_prompt_run(task_id, run_entry):\n",
    "    # Find the task file and append the run entry into its 'runs' list\n",
    "    candidates = list(TASKS_DIR.glob(f\"{int(task_id)}_*.json\"))\n",
    "    if not candidates:\n",
    "        return\n",
    "    p = candidates[0]\n",
    "    d = json.loads(p.read_text(encoding='utf-8'))\n",
    "    d.setdefault('runs', []).append(run_entry)\n",
    "    p.write_text(json.dumps(d, ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "def run_evaluation(models: list, strategies: list, examples: dict = None, save_prefix: str = 'results'):\n",
    "    if examples is None:\n",
    "        examples = EVAL_EXAMPLES\n",
    "\n",
    "    # Normalize and validate model selection\n",
    "    norm_models = validate_models(models)\n",
    "\n",
    "    # Preview experiments and warn if something is off\n",
    "    expected_runs, warnings = preview_experiments(TASKS, norm_models, strategies)\n",
    "    if warnings:\n",
    "        for w in warnings:\n",
    "            print('Warning:', w)\n",
    "    print(f'Running {expected_runs} experiment runs (skipping CoT for reasoning models)')\n",
    "\n",
    "    results = []\n",
    "    pbar = trange(expected_runs, desc='Running experiments')\n",
    "    for task in TASKS:\n",
    "        task_id = task['id']\n",
    "        ex = examples.get(task_id, {})\n",
    "        for model in norm_models:\n",
    "            for strategy in strategies:\n",
    "                # Skip CoT for reasoning-specialized models\n",
    "                if should_skip_cot(model, strategy):\n",
    "                    # record a skipped entry for traceability\n",
    "                    results.append({\n",
    "                        'task_id': task_id,\n",
    "                        'task_name': task['name'],\n",
    "                        'strategy': strategy,\n",
    "                        'model': model['name'],\n",
    "                        'prompt': None,\n",
    "                        'response': '',\n",
    "                        'elapsed': 0.0,\n",
    "                        'success': False,\n",
    "                        'error': 'CoT skipped for reasoning model',\n",
    "                        'timestamp': datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                model_name = model['name']\n",
    "                if strategy == 'zero':\n",
    "                    prompt = zero_shot_prompt(task, ex.get('input'))\n",
    "                elif strategy == 'few':\n",
    "                    prompt = few_shot_prompt(task, DEV_EXAMPLES.get(task_id, [])[:2], ex.get('input'))\n",
    "                elif strategy == 'cot':\n",
    "                    prompt = cot_prompt(task, ex.get('input'))\n",
    "                else:\n",
    "                    raise ValueError('Unknown strategy')\n",
    "\n",
    "                r = query_chat(model_name, prompt)\n",
    "                entry = {\n",
    "                    'task_id': task_id,\n",
    "                    'task_name': task['name'],\n",
    "                    'strategy': strategy,\n",
    "                    'model': model_name,\n",
    "                    'prompt': prompt,\n",
    "                    'response': r.get('response',''),\n",
    "                    'elapsed': r.get('elapsed', None),\n",
    "                    'success': r.get('success', False),\n",
    "                    'error': r.get('error', None),\n",
    "                    'timestamp': datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "                }\n",
    "                results.append(entry)\n",
    "\n",
    "                # Persist the prompt and response back into the per-task JSON file\n",
    "                save_prompt_run(task_id, {\n",
    "                    'timestamp': entry['timestamp'],\n",
    "                    'model': model_name,\n",
    "                    'strategy': strategy,\n",
    "                    'prompt': prompt,\n",
    "                    'response': entry['response'],\n",
    "                    'elapsed': entry['elapsed'],\n",
    "                    'success': entry['success'],\n",
    "                    'error': entry['error']\n",
    "                })\n",
    "\n",
    "                pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # Save results\n",
    "    ts = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')\n",
    "    out_json = OUTPUT_DIR / f\"{save_prefix}_{ts}.jsonl\"\n",
    "    with open(out_json, 'w', encoding='utf-8') as fh:\n",
    "        for r in results:\n",
    "            fh.write(json.dumps(r, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    # Save a CSV summary for manual annotation\n",
    "    df = pd.DataFrame(results)\n",
    "    csv_path = OUTPUT_DIR / f\"{save_prefix}_{ts}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"Saved {len(results)} results to {out_json} and {csv_path}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7efe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic metrics and aggregation\n",
    "def exact_match(pred: str, gold: str) -> bool:\n",
    "    if gold is None:\n",
    "        return False\n",
    "    # simple normalization\n",
    "    def norm(s):\n",
    "        return ''.join(c.lower() for c in s if c.isalnum())\n",
    "    return norm(pred) == norm(gold)\n",
    "\n",
    "def token_overlap(pred: str, gold: str) -> float:\n",
    "    if not gold or not pred:\n",
    "        return 0.0\n",
    "    def get_tokens(s):\n",
    "        return set(re.findall(r'\\w+', s.lower()))\n",
    "    \n",
    "    pred_tokens = get_tokens(pred)\n",
    "    gold_tokens = get_tokens(gold)\n",
    "    \n",
    "    if not gold_tokens:\n",
    "        return 0.0\n",
    "        \n",
    "    intersect = pred_tokens.intersection(gold_tokens)\n",
    "    # Recall-oriented: how many of the gold tokens did the model produce?\n",
    "    return len(intersect) / len(gold_tokens)\n",
    "\n",
    "def compute_metrics(results: list, examples: dict):\n",
    "    df = pd.DataFrame(results)\n",
    "    # add expected where available\n",
    "    df['expected'] = df['task_id'].map(lambda tid: examples.get(tid, {}).get('expected'))\n",
    "    \n",
    "    # Simple exact match\n",
    "    df['exact_match'] = df.apply(lambda r: exact_match(r['response'], r['expected']) if r['expected'] else None, axis=1)\n",
    "    \n",
    "    # Soft token overlap (better for open-ended answers)\n",
    "    df['overlap_score'] = df.apply(lambda r: token_overlap(r['response'], r['expected']) if r['expected'] else 0.0, axis=1)\n",
    "\n",
    "    # Compute per-task per-model-strategy metrics\n",
    "    agg = df.groupby(['task_id','task_name','model','strategy']).agg(\n",
    "        n=('response','size'),\n",
    "        n_exact=('exact_match', lambda x: sum(1 for v in x if v is True)),\n",
    "        avg_overlap=('overlap_score', 'mean')\n",
    "    ).reset_index()\n",
    "    agg['accuracy'] = agg['n_exact'] / agg['n']\n",
    "    return df, agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eda5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and strategies for the experiment\n",
    "selected_models = [\n",
    "    'SpeakLeash/bielik-1.5b-v3.0-instruct:Q8_0',\n",
    "    'ministral-3:3b',\n",
    "    'deepseek-r1:7b'\n",
    "]\n",
    "strategies = ['zero', 'few', 'cot']\n",
    "\n",
    "# Execute the evaluation\n",
    "# This will iterate through 10 tasks, 3 models, and up to 3 strategies per model.\n",
    "# Total expected runs: 10 * (2*3 + 1*2) = 10 * 8 = 80 runs.\n",
    "results = run_evaluation(\n",
    "    models=selected_models,\n",
    "    strategies=strategies,\n",
    "    examples=EVAL_EXAMPLES,\n",
    "    save_prefix='lab5_experiment'\n",
    ")\n",
    "\n",
    "# Compute and display metrics\n",
    "df_results, agg_metrics = compute_metrics(results, EVAL_EXAMPLES)\n",
    "\n",
    "print(\"\\n--- Evaluation Summary ---\")\n",
    "# Pivot the results for a better view: Models as columns, Tasks as rows\n",
    "summary_pivot = agg_metrics.pivot_table(\n",
    "    index=['task_id', 'task_name'], \n",
    "    columns=['model', 'strategy'], \n",
    "    values='accuracy'\n",
    ")\n",
    "display(summary_pivot)\n",
    "\n",
    "# Save the final aggregated metrics to a file for the report\n",
    "agg_metrics.to_csv(OUTPUT_DIR / \"final_experiment_results.csv\", index=False)\n",
    "print(f\"\\nFinal results saved to {OUTPUT_DIR / 'final_experiment_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c6b57",
   "metadata": {},
   "source": [
    "### LLM-as-a-Judge Evaluation\n",
    "Since `exact_match` is too strict for open-ended tasks, we use a reasoning model (`deepseek-r1:7b`) to evaluate the quality of the responses based on the task criteria. \n",
    "\n",
    "The judge will assign a score from 0 to 5. We then normalize this to a 0-1 scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_judge_score(task, prompt, response, judge_model='deepseek-r1:7b'):\n",
    "    \"\"\"Use an LLM to judge the quality of a response on a scale of 0-5.\"\"\"\n",
    "    if not response:\n",
    "        return 0.0\n",
    "        \n",
    "    criteria = task.get('eval_criteria', 'Correctness and relevance')\n",
    "    task_name = task.get('name', 'General Task')\n",
    "    \n",
    "    judge_prompt = f\"\"\"Evaluate the following LLM response based on the task description and evaluation criteria.\n",
    "    \n",
    "Task: {task_name}\n",
    "Criteria: {criteria}\n",
    "\n",
    "User Prompt: {prompt}\n",
    "LLM Response: {response}\n",
    "\n",
    "Give a score from 0 to 5, where:\n",
    "0: Completely irrelevant or incorrect\n",
    "1: Major issues, barely follows prompt\n",
    "2: Follows prompt but has significant errors\n",
    "3: Good response, minor issues\n",
    "4: Very good response, follows almost all criteria\n",
    "5: Perfect response\n",
    "\n",
    "Return ONLY the numeric score (0, 1, 2, 3, 4, or 5). Do not provide any explanation.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Use simple chat query\n",
    "        messages = [{'role': 'user', 'content': judge_prompt}]\n",
    "        res = client.chat(model=judge_model, messages=messages)\n",
    "        content = res['message']['content']\n",
    "        \n",
    "        # Remove reasoning blocks if present (DeepSeek-R1 style)\n",
    "        content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()\n",
    "        \n",
    "        # Find the first digit in the response\n",
    "        match = re.search(r'([0-5])', content)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "        return 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error judging response: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def run_judge_evaluation(results_df, tasks, judge_model='nemotron-3-nano:latest'):\n",
    "    print(f\"Judging {len(results_df)} responses using {judge_model}...\")\n",
    "    scores = []\n",
    "    \n",
    "    # Map tasks by ID for easy lookup\n",
    "    task_map = {t['id']: t for t in tasks}\n",
    "    \n",
    "    for _, row in tqdm(results_df.iterrows(), total=len(results_df)):\n",
    "        # Skip if error occurred during generation\n",
    "        if not row['success'] or not row['response']:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        task = task_map.get(row['task_id'], {})\n",
    "        score = get_judge_score(task, row['prompt'], row['response'], judge_model)\n",
    "        scores.append(score)\n",
    "        \n",
    "    results_df['judge_score'] = scores\n",
    "    results_df['normalized_judge_score'] = results_df['judge_score'] / 5.0\n",
    "    return results_df\n",
    "\n",
    "# Run the judge on our results\n",
    "# Note: To save time, you could run this on a subset or only for non-exact matches\n",
    "df_results = run_judge_evaluation(df_results, TASKS)\n",
    "\n",
    "# Re-compute aggregate metrics with judge scores\n",
    "agg_metrics = df_results.groupby(['task_id','task_name','model','strategy']).agg(\n",
    "    n=('response','size'),\n",
    "    avg_accuracy=('exact_match', 'mean'),\n",
    "    avg_overlap=('overlap_score', 'mean'),\n",
    "    avg_judge_score=('normalized_judge_score', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "display(agg_metrics.sort_values('avg_judge_score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the impact of prompting strategies using Judge Scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=agg_metrics, x='strategy', y='avg_judge_score', hue='model')\n",
    "plt.title('Impact of Prompting Strategy (Judge Score) across Models')\n",
    "plt.ylabel('Average Judge Score (Normalized 0-1)')\n",
    "plt.xlabel('Prompting Strategy')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"strategy_comparison_judge.png\")\n",
    "plt.show()\n",
    "\n",
    "# Performance by Task Type using Judge Scores\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=agg_metrics, x='task_name', y='avg_judge_score', hue='model')\n",
    "plt.title('Model Performance by Task Type (Judge Score)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Average Judge Score (Normalized 0-1)')\n",
    "plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"task_performance_judge.png\")\n",
    "plt.show()\n",
    "\n",
    "# Distribution Comparison\n",
    "plt.figure(figsize=(10, 5))\n",
    "melted_metrics = agg_metrics.melt(id_vars=['model'], value_vars=['avg_accuracy', 'avg_judge_score'], \n",
    "                                   var_name='Metric', value_name='Score')\n",
    "sns.boxplot(data=melted_metrics, x='Metric', y='Score')\n",
    "plt.title('Exact Match Accuracy vs. LLM Judge Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp-ling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
